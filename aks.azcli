#################################
# Created by Jose Moreno
# July 2020
#
# Some useful commands around AKS
#################################

# Variables
rg=akstest
location=eastus2
wait_interval=5s
flow_logs=yes
# AKS settings
aks_isolated=no
aks_private=yes
aks_private_mode=vnet   # plink/vnet
aks_name=aks
aks_rbac=yes
use_msi=yes
kubelet_identity=yes
aks_service_cidr=10.0.0.0/16
vm_size=Standard_B2ms         # Some possible values: Standard_B2ms, Standard_D2_v3
preview_version=yes
network_plugin=azure           # azure/kubenet/none/azure_overlay/azure_cilium
network_policy=none            # azure/calico/cilium/none
azure_cni_pod_subnet=no        # yes/no
az_monitor=no
# VNet
vnet_name=aksVnet
vnet_prefix=10.13.0.0/16
aks_subnet_name=aks1stpool
aks_subnet_prefix=10.13.76.0/24  # Min /25 with Azure CNI!
aks2_subnet_name=aks2ndpool
aks2_subnet_prefix=10.13.75.0/24
aks2_pod_subnet_name=aks2ndpoolpods
aks2_pod_subnet_prefix=10.13.65.0/24
pod_subnet_name=pods
pod_subnet_prefix=10.13.80.0/24
aks_api_subnet_name=aksapi
aks_api_subnet_prefix=10.13.81.0/24
vm_subnet_name=vm
vm_subnet_prefix=10.13.1.0/24
bastion_subnet_name=AzureBastionSubnet 
bastion_subnet_prefix=10.13.15.0/24
bastion_name=aksbastion
gw_subnet_prefix=10.13.0.0/24
appgw_subnet_name=AppGateway
appgw_subnet_prefix=10.13.10.0/24
azfw_subnet_prefix=10.13.11.0/24
apim_subnet_prefix=10.13.12.0/24
db_subnet_prefix=10.13.50.0/24
akslb_subnet_prefix=10.13.77.0/24
arm_subnet_prefix=10.13.79.0/24
aci_subnet_name=aci
aci_subnet_prefix=10.13.100.0/24
ep_subnet_name=ep
ep_subnet_prefix=10.13.101.0/24
vm_name=testlinuxvm
vm_size=Standard_B2ms
gw_name=${vnet_name}-gw
gw_pip_name=${vnet_name}-gw-pip
p2s_address_pool='100.64.0.0/24'
p2s_config_file="$HOME/downloads/$(date +%Y%m%d)-vpnclientpackage.zip"
docker_username='your_dockerhub_username'
docker_pat='your_dockerhub_pat'
# Other resources (not used)
# kv_name=erjositoKeyvault
# acr_name=erjositoAcr

####################
# Helper functions #
####################

# Wait for resource to be created
function wait_until_finished {
     wait_interval=15
     resource_id=$1
     resource_name=$(echo $resource_id | cut -d/ -f 9)
     echo "INFO: Waiting for resource $resource_name to finish provisioning..."
     start_time=`date +%s`
     state=$(az resource show --id $resource_id --query properties.provisioningState -o tsv)
     until [[ "$state" == "Succeeded" ]] || [[ "$state" == "Failed" ]] || [[ -z "$state" ]]
     do
        sleep $wait_interval
        state=$(az resource show --id $resource_id --query properties.provisioningState -o tsv)
     done
     if [[ -z "$state" ]]
     then
        echo "ERROR: Something really bad happened..."
     else
        run_time=$(expr `date +%s` - $start_time)
        ((minutes=${run_time}/60))
        ((seconds=${run_time}%60))
        echo "INFO: Resource $resource_name provisioning state is $state, wait time $minutes minutes and $seconds seconds"
     fi
}

###################
# Enable features #
###################

function enableAksFeature () {
    feature_name=$1
    state=$(az feature list -o table --query "[?contains(name, 'microsoft.containerservice/$feature_name')].properties.state" -o tsv)
    if [[ "$state" == "Registered" ]]
    then
        echo "INFO: $feature_name is already registered"
    else
        echo "INFO: Registering feature $feature_name..."
        az feature register --name "$feature_name" --namespace microsoft.containerservice -o none --only-show-errors
        state=$(az feature list -o table --query "[?contains(name, 'microsoft.containerservice/$feature_name')].properties.state" -o tsv)
        echo "INFO: Waiting for feature $feature_name to finish registering..."
        wait_interval=15
        until [[ "$state" == "Registered" ]]
        do
            sleep $wait_interval
            state=$(az feature list -o table --query "[?contains(name, 'microsoft.containerservice/$feature_name')].properties.state" -o tsv)
            echo "INFO: Current registration status for feature $feature_name is $state"
        done
        echo "INFO: Registering resource provider Microsoft.ContainerService now..."
        az provider register --namespace Microsoft.ContainerService -o none --only-show-errors
    fi
}

# enableAksFeature "AKS-IngressApplicationGatewayAddon"
# enableAksFeature "EnablePodIdentityPreview"
# enableAksFeature "MigrateToMSIClusterPreview"
# enableAksFeature "PodSubnetPreview"
enableAksFeature "EnableAPIServerVnetIntegrationPreview"
# enableAksFeature "CiliumDataplanePreview"
# enableAksFeature "AzureServiceMeshPreview"
# enableAksFeature "NetworkObservabilityPreview"

# Update extension
echo "INFO: Updating aks-preview extension..."
az extension update -n aks-preview

########
# Main #
########

# Create RG, LA workspace, vnet, AKS
echo "INFO: Creating RG and VNet..."
az group create -n $rg -l $location -o none
acr_rg=$(az acr list -o tsv --query "[?name=='$acr_name'].resourceGroup") && echo "INFO: ACR RG is '$acr_rg'"
if [[ -n "$acr_rg" ]]; then
    acr_id=$(az acr show -n erjositoAcr -g $acr_rg --query id -o tsv)
fi
az network vnet create -g $rg -n $vnet_name --address-prefix $vnet_prefix -l $location -o none
az network vnet subnet create -g $rg -n $aks_subnet_name --vnet-name $vnet_name --address-prefix $aks_subnet_prefix --default-outbound true -o none
az network vnet subnet create -g $rg -n $pod_subnet_name --vnet-name $vnet_name --address-prefix $pod_subnet_prefix --default-outbound true -o none
az network vnet subnet create -g $rg -n $aks_api_subnet_name --vnet-name $vnet_name --address-prefix $aks_api_subnet_prefix -o none
az network vnet subnet create -g $rg -n $ep_subnet_name --vnet-name $vnet_name --address-prefix $ep_subnet_prefix -o none
az network vnet subnet create -g $rg -n $vm_subnet_name --vnet-name $vnet_name --address-prefix $vm_subnet_prefix -o none
aks_subnet_id=$(az network vnet subnet show -n $aks_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
pod_subnet_id=$(az network vnet subnet show -n $pod_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)

# Create LA workspace
logws_name=$(az monitor log-analytics workspace list -g $rg --query '[0].name' -o tsv)
if [[ -z "$logws_name" ]]
then
    logws_name=log$RANDOM
    echo "INFO: Creating log analytics workspace ${logws_name}..."
    az monitor log-analytics workspace create -n $logws_name -g $rg -o none
else
    echo "INFO: Log Analytics workspace $logws_name found in resource group $rg"
fi
logws_id=$(az resource list -g $rg -n $logws_name --query '[].id' -o tsv)
logws_customerid=$(az monitor log-analytics workspace show -n $logws_name -g $rg --query customerId -o tsv)

# Flow logs in VNet
if [[  "$flow_logs" == "yes" ]]; then
    storage_account_name=$(az storage account list -g $rg -o tsv --query '[0].name' 2>/dev/null | head -1)
    if [[ -z "$storage_account_name" ]]; then
        storage_account_name="logs$RANDOM"  # max 24 characters
        echo "INFO: No storage account found in resource group $rg, creating one..."
        az storage account create -n $storage_account_name -g $rg --sku Standard_LRS --kind StorageV2 -l $location -o none
    else
        echo "INFO: Storage account $storage_account_name found in $location, using it for VNet flow flogs"
    fi
    echo "INFO: Enabling VNet Flow Logs for VNet $vnet_name in storage account $storage_account_name and Log Analytics workspace $logws_name..."
    if [[ -z "$logws_name" ]]; then
        echo "ERROR: Log Analytics workspace is not defined, cannot enable traffic analytics"
    else
        az network watcher flow-log create -l $location -g $rg --name "aks-${aks_name}-${location}" --vnet $vnet_name \
          --storage-account $storage_account_name --workspace $logws_name --interval 10 --traffic-analytics true -o none
    fi
fi

# Get latest supported/preview version
# k8s_versions=$(az aks get-versions -l $location -o json --only-show-errors)
# if [[ "$preview_version" == "yes" ]]
# then
#     k8s_version=$(echo $k8s_versions | jq -r '.values | map(select(.isPreview == true))[0] | .version')
#     echo "INFO: Latest supported k8s version is $k8s_version (in preview)"
# else
#     k8s_version=$(echo $k8s_versions | jq -r '.values | map(select(.isPreview == null))[0] | .version')
#     echo "INFO: Latest supported k8s version (not in preview) is $k8s_version"
# fi

# Setting identity flags (managed identity or SP)
if [[ "$use_msi" == no ]]
then
    # Get SP from AKV
    keyvault_name=joseakv-airs
    purpose=aks
    keyvault_appid_secret_name=$purpose-sp-appid
    keyvault_password_secret_name=$purpose-sp-secret
    sp_app_id=$(az keyvault secret show --vault-name $keyvault_name -n $keyvault_appid_secret_name --query 'value' -o tsv) && echo $sp_app_id
    sp_app_secret=$(az keyvault secret show --vault-name $keyvault_name -n $keyvault_password_secret_name --query 'value' -o tsv)

    # Assign contributor role to the vnet
    vnet_id=$(az network vnet show -n $vnet_name -g $rg --query id -o tsv)
    az role assignment create --scope $vnet_id --assignee $sp_app_id --role Contributor
    # az aks create flags
    identity_options="--service-principal $sp_app_id --client-secret $sp_app_secret --skip-subnet-role-assignment"
else
    # User identity
    id_name=aksid
    id_id=$(az identity show -n $id_name -g $rg --query id -o tsv 2>/dev/null)
    if [[ -z "$id_id" ]]
    then
        echo "INFO: Identity $id_name not found, creating a new one..."
        az identity create -n $id_name -g $rg -o none
        id_id=$(az identity show -n $id_name -g $rg --query id -o tsv)
    else
        echo "INFO: Identity $id_name found with ID $id_id"
    fi
    id_principal_id=$(az identity show -n $id_name -g $rg --query principalId -o tsv)
    vnet_id=$(az network vnet show -n $vnet_name -g $rg --query id -o tsv)
    sleep 15 # Time for creation to propagate
    echo "INFO: Assigning contributor role for identity $id_name on VNet $vnet_name..."
    az role assignment create --scope $vnet_id --assignee $id_principal_id --role Contributor -o none
    # Kubelet identity
    if [[ "$kubelet_identity" == "yes" ]]; then
        kid_name="${id_name}-kubelet"
        kid_id=$(az identity show -n $kid_name -g $rg --query id -o tsv 2>/dev/null)
        if [[ -z "$kid_id" ]]
        then
            echo "INFO: Kubelet identity $kid_name not found, creating a new one..."
            az identity create -n $kid_name -g $rg -o none
            kid_id=$(az identity show -n $kid_name -g $rg --query id -o tsv)
        else
            echo "INFO: Kubelet identity $kid_name found with ID $kid_id"
        fi
        kid_principal_id=$(az identity show -n $kid_name -g $rg --query principalId -o tsv)
        identity_options="--enable-managed-identity --assign-identity $id_id --assign-kubelet-identity $kid_id"
    else
      identity_options="--enable-managed-identity --assign-identity $id_id"
    fi
    # System identity
    # identity_options="--enable-managed-identity"
fi

# Change network policy format if none specified
if [[ "$network_policy" == 'none' ]]; then
    network_policy="''"
fi

# CNI options
aks_subnet_id=$(az network vnet subnet show -n $aks_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
pod_subnet_id=$(az network vnet subnet show -n $pod_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
if [[ "$network_plugin" == 'azure' ]]; then
    if [[ "$azure_cni_pod_subnet" == "yes" ]]; then
      cni_options="--network-plugin azure --pod-subnet-id $pod_subnet_id"
    else
      cni_options="--network-plugin azure"
    fi
elif [[ "$network_plugin" == 'kubenet' ]]; then
    cni_options="--network-plugin kubenet"
elif [[ "$network_plugin" == 'azure_cilium' ]]; then
    cni_options="--network-plugin azure --network-dataplane cilium --pod-subnet-id $pod_subnet_id"
    if [[ "$network_policy" != "cilium" ]]; then
        echo "ERROR: Network policy must be cilium when using azure_cilium. Setting network policy to cilium..."
        network_policy=cilium
    fi
elif [[ "$network_plugin" == 'azure_overlay' ]]; then
    cni_options="--network-plugin azure --network-dataplane cilium --network-plugin-mode overlay"
    if [[ "$network_policy" != "cilium" ]]; then
        echo "ERROR: Network policy must be cilium when using azure_cilium. Setting network policy to cilium..."
    fi
    network_policy=cilium
else
    echo "ERROR: Network plugin $network_plugin not supported"
    exit 1
fi
if [[ -n "$network_policy" ]]; then
    cni_options+=" --network-policy $network_policy"
fi

# Create VPN gateway if isolated
if [[ "$aks_isolated" == "yes" ]]; then
    echo "INFO: Creating VPN gateway for isolated AKS cluster (using '--no-wait' flag)..."
    az network public-ip create -g $rg -n $gw_pip_name --sku Standard --allocation-method Static -o none
    az network vnet subnet create -g $rg -n GatewaySubnet --vnet-name $vnet_name --address-prefix $gw_subnet_prefix -o none
    az network vnet-gateway create -g $rg -n $gw_name --public-ip-address $gw_pip_name --vnet $vnet_name --sku VpnGw1 -o none --no-wait
fi

# Create DNS server if isolated or private cluster
if [[ "$aks_isolated" == "yes" ]] || [[ "$aks_private" == "yes" ]]; then
        # DNS Server
        echo "INFO: creating DNS server VM '${vm_name}' in subnet '${vm_subnet_name}'..."
        vm_pip_name="${vm_name}-pip"
        az vm create -n $vm_name -g $rg --image Ubuntu2204 --generate-ssh-keys --size $vm_size -l $location \
           --vnet-name $vnet_name --subnet $vm_subnet_name --public-ip-address $vm_pip_name --public-ip-sku Standard -o none
        vm_pip=$(az network public-ip show -n $vm_pip_name -g $rg --query ipAddress -o tsv)
        vm_nic_id=$(az vm show -n $vm_name -g $rg --query 'networkProfile.networkInterfaces[0].id' -o tsv)
        vm_privateip=$(az network nic show --ids $vm_nic_id --query 'ipConfigurations[0].privateIPAddress' -o tsv)
        echo "DNS server deployed to $vm_privateip, $vm_pip"
        echo "Installing dnsmasq in VM ${vm_name}..."
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo apt update -y && sudo apt -y install dnsmasq"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo sed -i '$ a\log-queries' /etc/dnsmasq.conf"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo sed -i '/\-\-local\-service/s/^/#/' /etc/init.d/dnsmasq"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo systemctl disable systemd-resolved"
        # From now on until DNS is fully configured, commands will be very slow, as DNS is not working
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no "$vm_pip" "sudo sed -i \"\$ a 168.63.129.16 dnsserver\" /etc/hosts"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo systemctl stop systemd-resolved"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo unlink /etc/resolv.conf"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "echo nameserver 168.63.129.16 | sudo tee /etc/resolv.conf"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo systemctl restart dnsmasq"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo systemctl enable dnsmasq"
        ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip "sudo systemctl status dnsmasq"
fi

# Create ACR with private link if isolated or private cluster and set the options
if [[ "$aks_isolated" == "yes" ]] && [[ "$aks_private" == "yes" ]]; then
    acr_name=$(az acr list -g $rg -o tsv --query '[0].name' 2>/dev/null)
    if [[ -n "$acr_name" ]]; then
        echo "INFO: ACR $acr_name found in resource group $rg, using it for isolated AKS cluster"
        acr_id=$(az acr show -n $acr_name -g $rg --query id -o tsv)
    else
        # ACR
        acr_name="${aks_name}${RANDOM}"
        echo "INFO: Creating ACR $acr_name for isolated AKS cluster..."
        az acr create -n $acr_name -g $rg  --sku Premium --public-network-enabled false -o none
        echo "INFO: Configuring cache for ACR $acr_name..."
        az acr cache create -n aks-managed-mcr -r $acr_name -g $rg --source-repo "mcr.microsoft.com/*" --target-repo "aks-managed-repository/*" -o none
        echo "INFO: configuring diagnostic settings for ACR $acr_name..."
        acr_id=$(az acr show -n $acr_name -g $rg --query id -o tsv)
        az monitor diagnostic-settings create -n mydiag --resource $acr_id --workspace $logws_id \
                --logs '[{"category": "ContainerRegistryRepositoryEvents", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
                        {"category": "ContainerRegistryLoginEvents", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}]' -o none
        # Private endpoint
        echo "INFO: Creating private endpoint for ACR $acr_name..."
        pe_name=${acr_name}-pe
        az network private-endpoint create -n $pe_name -g $rg --vnet-name $vnet_name --subnet $ep_subnet_name \
          --private-connection-resource-id $acr_id --group-id registry --connection-name myConnection -o none
        echo "INFO: Configuring DNS for private endpoint $pe_name..."
        zone_name="privatelink.azurecr.io"
        az network private-dns zone create -g $rg -n $zone_name -o none
        az network private-dns link vnet create -g $rg --zone-name $zone_name -n $vnet_name --virtual-network $vnet_name --registration-enabled false -o none
        az network private-endpoint dns-zone-group create --endpoint-name $pe_name -g $rg -n myzonegroup --zone-name zone1 --private-dns-zone $zone_name -o none
        # Kubelet identity
        if [[ "$use_msi" == "yes" ]]; then
            if [[ "$kubelet_identity" == "yes" ]] && [[ -n "$kid_principal_id" ]]; then
                echo "INFO: Assigning AcrPull role for kubelet identity $kid_name on ACR $acr_name..."
                az role assignment create --assignee $kid_principal_id --role AcrPull --scope $acr_id -o none
            else
                echo "ERROR: You need to use both use_msi=yes and kubelet_identity=yes to assign AcrPull role to the kubelet identity"
            fi
        else
            echo "ERROR: You need to use both use_msi=yes and kubelet_identity=yes to assign AcrPull role to the kubelet identity"
        fi
    fi
    # AKS isolated options
    api_subnet_id=$(az network vnet subnet show -n $aks_api_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
    if [[ "$aks_private_mode" == "vnet" ]]; then
        isolated_options="--bootstrap-artifact-source Cache --bootstrap-container-registry-resource-id $acr_id --outbound-type none --enable-private-cluster --enable-apiserver-vnet-integration --apiserver-subnet-id $api_subnet_id"
    elif [[ "$aks_private_mode" == "plink" ]]; then
        isolated_options="--bootstrap-artifact-source Cache --bootstrap-container-registry-resource-id $acr_id --outbound-type none --enable-private-cluster --private-dns-zone System --disable-public-fqdn"
    fi
else
    isolated_options=""
fi

# Private cluster options (not isolated)
if [[ "$aks_isolated" == "no" ]] && [[ "$aks_private" == "yes" ]]; then
    if [[ "$aks_private_mode" == "plink" ]]; then
        isolated_options="--enable-private-cluster --private-dns-zone System --disable-public-fqdn"
    elif [[ "$aks_private_mode" == "vnet" ]]; then
        isolated_options="--enable-private-cluster --enable-apiserver-vnet-integration --apiserver-subnet-id $api_subnet_id"
    else
        echo "ERROR: aks_private_mode $aks_private_mode not supported"
        exit 1
    fi
fi

# Error control because the code to get the k8s version is broken
if [[ -z "$k8s_version" ]]; then
  k8s_version_options=""
else
  k8s_version_options="-k $k8s_version"
fi

# Create AKS
echo "INFO: Deploying cluster ${aks_name}..."
# echo "DEBUG: Identity options  : $identity_options"
# echo "DEBUG: Networking options: $cni_options"
# echo "DEBUG: Isolated options  : $isolated_options"
az aks create -g $rg -n $aks_name -l $location -o none --only-show-errors \
    -c 1 -s $vm_size --generate-ssh-keys -u $(whoami) \
    ${(z)identity_options} \
    ${(z)cni_options} \
    ${(z)isolated_options} \
    --vnet-subnet-id $aks_subnet_id --service-cidr $aks_service_cidr \
    --load-balancer-sku Standard \
    --node-resource-group "$aks_name"-iaas-"$RANDOM" \
    --dns-name-prefix cloudtrooper --no-wait

# Other options you can use in the previous command
    # ${(z)k8s_version_options} \
    # --enable-private-cluster --private-dns-zone none --disable-public-fqdn \
    # --enable-apiserver-vnet-integration \
    # --enable-cilium-dataplane  \   # https://techcommunity.microsoft.com/t5/azure-networking-blog/azure-cni-powered-by-cilium-for-azure-kubernetes-service-aks/ba-p/3662341
    # --pod-subnet-id $pod_subnet_id \
    # --enable-pod-security-policy \  # Deprecated
    # --enable-cluster-autoscaler --min-count 1 --max-count 3 \
    # --cluster-autoscaler-profile scan-interval=30s \
    # --dns-name-prefix cloudtrooper \
    # --node-osdisk-type ephemeral --node-osdisk-size 30 \
    # --outbound-type userDefinedRouting \
    # --aks-custom-headers EnableAzureDiskFileCSIDriver=true \
    # --os-sku Mariner \
    # --uptime-sla \
    # --network-policy 'calico' \
    # --no-wait

########
# Wait #
########

aks_id=$(az aks show -n $aks_name -g $rg --query id -o tsv)
wait_until_finished $aks_id

# Get credentials for kubectl
az aks list -o table
az aks get-credentials -n $aks_name -g $rg --overwrite
kubectl get nodes

# Finish VPN GW (to be tested)
if [[ "$aks_isolated" == "yes" ]]; then
    gw_id=$(az network vnet-gateway show -n $gw_name -g $rg --query id -o tsv)
    wait_until_finished $gw_id
    echo "INFO: VPN gateway for isolated AKS cluster is ready. Creating P2S configuration..."
    aad_audience="c632b3df-fb67-4d84-bdcf-b95ad541b5c8"
    tenant_id=$(az account show --query tenantId -o tsv)
    aad_issuer="https://sts.windows.net/${tenant_id}/"
    aad_tenant="https://login.microsoftonline.com/${tenant_id}/"
    az network vnet-gateway update -g $rg -n $gw_name --address-prefixes $p2s_address_pool --client-protocol OpenVPN -o none
    az network vnet-gateway aad assign --gateway-name $gw_name -g $rg --tenant "$aad_tenant" --audience "$aad_audience" --issuer "$aad_issuer" -o none
    echo "INFO: Creating VPN client configuration package..."
    az network vnet-gateway vpn-client generate -n $gw_name -g $rg --processor-architecture Amd64 -o none
    download_url=$(az network vnet-gateway vpn-client show-url -n $gw_name -g $rg -o tsv)
    p2s_config_file_abs=$(readlink -f "$p2s_config_file")
    curl -s -o $p2s_config_file_abs $download_url
    unzip -o $p2s_config_file_abs
    echo "INFO: VPN client configuration downloaded to $p2s_config_file_abs and unzipped"
fi

######################
# Modify the cluster #
######################

# Enable monitoring addon
if [[ "$az_monitor" == 'yes' ]]; then
    az aks enable-addons -g $rg -n $aks_name --addons monitoring --workspace-resource-id "$logws_id"
fi

# Enable AKV addon (Work In Progress)
az aks enable-addons -g $rg -n $aks_name --addons azure-keyvault-secrets-provider

# Add cluster autoscaler (requires the monitoring addon)
az aks update -g $rg -n $aks_name --enable-cluster-autoscaler --min-count 1 --max-count 4

# Modify autoscaler profile (see https://docs.microsoft.com/azure/aks/cluster-autoscaler#using-the-autoscaler-profile)
az aks update -g $rg -n $aks_name --cluster-autoscaler-profile scale-down-unneeded-time=1m

# Enable virtual node
az network vnet subnet create -g $rg -n $aci_subnet_name --vnet-name $vnet_name --address-prefix $aci_subnet_prefix
az aks enable-addons --addons virtual-node -n $aks_name -g $rg --subnet-name $aci_subnet_name

# Enable Azure Policy
az aks enable-addons --addons azure-policy -n $aks_name -g $rg

# Add diag settings for cluster logs
aks_id=$(az aks show -n $aks_name -g $rg --query id -o tsv)
az monitor diagnostic-settings create -n mydiag --resource $aks_id --workspace $logws_id \
    --metrics '[{"category": "AllMetrics", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false }, "timeGrain": null}]' \
    --logs '[{"category": "kube-apiserver", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}, 
            {"category": "kube-audit", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
            {"category": "kube-audit-admin", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
            {"category": "kube-controller-manager", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
            {"category": "kube-scheduler", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
            {"category": "cluster-autoscaler", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
            {"category": "guard", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}]'

# Migrate to MSI
az aks update -g $rg -n $aks_name --enable-managed-identity -y

# Update to new version
new_aks_version=1.21.2
az aks upgrade -n $aks_name -g $rg -k $new_aks_version -y

################################
#  Managed Prometheus/Grafana  #
################################

# Variables
grafana_name="${aks_name}-grafana-${RANDOM}"
monitor_ws_name="${aks_name}-monitor-${RANDOM}"

# Create Grafana/Prometheus
az grafana create -n $grafana_name -g $rg -o none
az monitor account create -n $monitor_ws_name -g $rg -l $location -o none

grafana_id=$(az grafana show -n $grafana_name -g $rg -o none --query id -o tsv)
monitor_ws_id=$(az monitor account show -n $monitor_ws_name -g $rg -o none --query id -o tsv)
az aks update --disable-azure-monitor-metrics -n $aks_name -g $rg -o none
az aks update --enable-azure-monitor-metrics -n $aks_name -g $rg --azure-monitor-workspace-resource-id $monitor_ws_id --grafana-resource-id  $grafana_id -o none

# Browse
grafana_url=$(az grafana show -n $grafana_name -g $rg -o tsv --query properties.endpoint)
echo "INFO: Grafana URL is $grafana_url"

# Verify
kubectl get po -owide -n kube-system | grep ama-
kubectl get ds ama-metrics-node --namespace=kube-system

##################
#     Retina     #
##################

# https://retina.sh/docs/installation/setup

# Basic
VERSION=$( curl -sL https://api.github.com/repos/microsoft/retina/releases/latest | jq -r .name)
helm upgrade --install retina oci://ghcr.io/microsoft/retina/charts/retina \
    --version $VERSION \
    --namespace kube-system \
    --set image.tag=$VERSION \
    --set operator.tag=$VERSION \
    --set logLevel=info \
    --set enabledPlugin_linux="\[dropreason\,packetforward\,linuxutil\,dns\]"

# Advanced with local context (with capture support)
VERSION=$( curl -sL https://api.github.com/repos/microsoft/retina/releases/latest | jq -r .name)
helm upgrade --install retina oci://ghcr.io/microsoft/retina/charts/retina \
    --version $VERSION \
    --namespace kube-system \
    --set image.tag=$VERSION \
    --set operator.tag=$VERSION \
    --set image.pullPolicy=Always \
    --set logLevel=info \
    --set os.windows=true \
    --set operator.enabled=true \
    --set operator.enableRetinaEndpoint=true \
    --skip-crds \
    --set enabledPlugin_linux="\[dropreason\,packetforward\,linuxutil\,dns\,packetparser\]" \
    --set enablePodLevel=true \
    --set enableAnnotations=true

# Uninstall
helm -n kube-system uninstall retina


# Import Retina dashboard to grafana
# dashboard_id=16611   # From https://learn.microsoft.com/en-us/azure/aks/network-observability-managed-cli?tabs=cilium#enable-visualization-on-grafana
dashboard_id=18814    # From https://retina.sh/docs/installation/grafana
az grafana dashboard import -g $rg -n $grafana_name --definition $dashboard_id -o none
# 412 Client Error: Precondition Failed for url: https://aks-grafana-5436-bvh4e6hagxfka2h4.eus2.grafana.azure.com/api/dashboards/import

####################
#  Install Cilium  #
####################

if [[ "$network_plugin" == 'none' ]]; then
    CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/master/stable.txt)
    CLI_ARCH=amd64
    if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
    curl -L --fail --remote-name-all "https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}"
    sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
    sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
    rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}

    # Deploy Cilium
    cilium install --azure-resource-group $rg

    # Check status
    cilium status
    cilium connectivity test
    k get no
fi

#######################
# Troubleshoot Cilium #
#######################

kubectl -n kube-system get pods -l k8s-app=cilium
cilium version
cilium status
cilium status --verbose
k get cep
kubectl get cep -o jsonpath='{range .items[*]}{@.status.id}{"="}{@.status.status.policy.spec.policy-enabled}{"\n"}{end}'


# From https://github.com/cilium/cilium/blob/main/contrib/k8s/k8s-cilium-exec.sh
function get_cilium_pods {
    kubectl -n kube-system get pods -l k8s-app=cilium -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName | \
       grep cilium
}
function run_command {
    while read -r podName nodeName ; do
      (
        title="==== Detail from pod ${podName}, on node ${nodeName}"
        msg=$( kubectl -n "kube-system" exec -c "cilium-agent" "${podName}" -- "${@}" 2>&1 )
        echo -e "$title \n$msg\n"
      )&
    done <<< "$(get_cilium_pods)"
    wait
}
run_command cilium status
run_command cilium-health status

# Cilium container logs:
# level=info msg="Imported CiliumNetworkPolicy" ciliumNetworkPolicyName=api-netpol k8sApiVersion= k8sNamespace=default subsys=k8s-watcher
# level=info msg="Policy imported via API, recalculating..." policyAddRequest=512e7deb-c02c-4e5a-b9bd-79481ebee0af policyRevision=19 subsys=daemon
# level=info msg="Rewrote endpoint BPF program" containerID= datapathPolicyRevision=18 desiredPolicyRevision=19 endpointID=644 identity=23553 ipv4= ipv6= k8sPodName=/ subsys=endpoint

# Monitor
label="run=api"
workload_pod_name=$(kubectl get pods -l $label -o jsonpath='{.items[0].metadata.name}')
workload_node_name=$(kubectl get pods -l $label -o jsonpath='{.items[0].spec.nodeName}')
cilium_pod_name=$(kubectl -n kube-system get pods -l k8s-app=cilium -o jsonpath="{.items[?(@.spec.nodeName=='$workload_node_name')].metadata.name}")
apipod_ip=$(kubectl get pods -l run=api -o jsonpath='{.items[0].status.podIP}')

# See https://docs.cilium.io/en/stable/cmdref/cilium_monitor/
kubectl -n kube-system exec -ti $cilium_pod_name -- cilium monitor
kubectl -n kube-system exec -ti $cilium_pod_name -- cilium monitor --type drop
kubectl -n kube-system exec -ti $cilium_pod_name -- cilium monitor --from 567
kubectl -n kube-system exec -ti $cilium_pod_name -- cilium monitor --from $apipod_ip --type drop
kubectl -n kube-system exec -ti $cilium_pod_name -- cilium monitor | grep -v 168.63.129.16 | grep "$apipod_ip" | grep "$webpod_ip"

# Sample outputs
# xx drop (Policy denied) flow 0x48bbc6f0 to endpoint 644, file bpf_lxc.c line 2001, , identity world->23553: 168.63.129.16:53042 -> 10.13.80.36:8080 tcp SYN
# xx drop (Policy denied) flow 0x290953e8 to endpoint 0, file bpf_lxc.c line 1182, , identity 23553->11944: 10.13.80.36:35161 -> 10.13.80.15:53 udp
# xx drop (Policy denied) flow 0xaccce700 to endpoint 0, file bpf_lxc.c line 1182, , identity 23553->world: 10.13.80.36:49057 -> 13.71.193.33:1433 tcp SYN
# xx drop (Policy denied) flow 0xd7fa1fe8 to endpoint 0, file bpf_lxc.c line 1182, , identity 23553->world: 10.13.80.36:52243 -> 13.78.248.58:11039 tcp SYN

# Finding pod ID (from ceps)
apipod_id=$(k get cep -l run=api -o jsonpath='{.items[0].status.id}') && echo "INFO: API pod has ID $apipod_id"
# Finding pod ID (from the cilium pod)
kubectl -n kube-system exec -ti $cilium_pod_name -- cilium endpoint list -o jsonpath='{range [*]}{.id}{"\t"}{.status.external-identifiers.pod-name}{"\n"}{end}'
ns=default
apipod_name=$(kubectl get pods -n $ns -l run=api -o jsonpath='{.items[0].metadata.name}')
apipod_id=$(kubectl -n kube-system exec -ti $pod_name -- cilium endpoint list -o jsonpath="{[?(@.status.external-identifiers.pod-name=='$ns/$apipod_name')].id}") && echo $apipod_id
kubectl -n kube-system exec -ti $cilium_pod_name -- cilium monitor --from $apipod_id --type drop



# From cilium pod
kubectl -n kube-system exec -ti $cilium_pod_name -- bash
root@aks-nodepool1-34838928-vmss000000:/home/cilium# cilium endpoint get 644 -o jsonpath='{range ..status.policy.realized.l4.ingress[*].derived-from-rules}{@}{"\n"}{end}'|tr -d ']['
"k8s:io.cilium.k8s.policy.derived-from=CiliumNetworkPolicy","k8s:io.cilium.k8s.policy.name=api-netpol","k8s:io.cilium.k8s.policy.namespace=default","k8s:io.cilium.k8s.policy.uid=ddd4a267-38c1-411a-883f-3f52a176cfc5"
root@aks-nodepool1-34838928-vmss000000:/home/cilium# cilium policy get k8s:io.cilium.k8s.policy.uid=ddd4a267-38c1-411a-883f-3f52a176cfc5
root@aks-nodepool1-34838928-vmss000000:/home/cilium# cilium endpoint get 644 -o jsonpath='{range ..status.policy.realized.l4.egress[*].derived-from-rules}{@}{"\n"}{end}'|tr -d ']['
"k8s:io.cilium.k8s.policy.derived-from=CiliumNetworkPolicy","k8s:io.cilium.k8s.policy.name=api-netpol","k8s:io.cilium.k8s.policy.namespace=default","k8s:io.cilium.k8s.policy.uid=ddd4a267-38c1-411a-883f-3f52a176cfc5"
"k8s:io.cilium.k8s.policy.derived-from=CiliumNetworkPolicy","k8s:io.cilium.k8s.policy.name=api-netpol","k8s:io.cilium.k8s.policy.namespace=default","k8s:io.cilium.k8s.policy.uid=ddd4a267-38c1-411a-883f-3f52a176cfc5"

#####################
#   Egress Gateway  #
#####################

az aks nodepool add --cluster-name $aks_name -g $rg -n egressgws --node-count 1 -s $vm_size --mode User \
    --node-taints "kubeegressgateway.azure.com/mode=true:NoSchedule" --labels "kubeegressgateway.azure.com/mode=true" -o none

# Prepare Helm chart values file (using MSI created for the cluster)
config_file='/tmp/azure_config.yaml'

# Config file for MSI
url=https://raw.githubusercontent.com/Azure/kube-egress-gateway/main/docs/samples/sample_azure_config_msi.yaml
# config:
#   azureCloudConfig:
#     cloud: "AzurePublicCloud"
#     tenantId: "<tenant ID>"
#     subscriptionId: "<subscription ID>"
#     useManagedIdentityExtension: true
#     userAssignedIdentityID: "<user assigned msi clientID>"
#     userAgent: "kube-egress-gateway-controller"
#     resourceGroup: "<resource group name>"
#     location: "<resource group location>"
#     gatewayLoadBalancerName: "kubeegressgateway-ilb"
#     loadBalancerResourceGroup: ""
#     vnetName: "<virtual network name>"
#     vnetResourceGroup: ""
#     subnetName: "<subnet name>"
wget $url -q -O $config_file
tenant_id=$(az account show -o tsv --query tenantId)
subscription_id=$(az account show --query id -o tsv)
id_id=$(az identity show -g $rg -n $id_name -o tsv --query id)
sed -i "s|<tenant ID>|${tenant_id}|g" $config_file
sed -i "s|<subscription ID>|${subscription_id}|g" $config_file
sed -i "s|<resource group name>|${rg}|g" $config_file
sed -i "s|<virtual network name>|${vnet_name}|g" $config_file
sed -i "s|<subnet name>|${aks_subnet_name}|g" $config_file
sed -i "s|<user assigned msi clientID>|${id_id}|g" $config_file
sed -i "s|<resource group location>|${location}|g" $config_file

# Config file for SP
url=https://raw.githubusercontent.com/Azure/kube-egress-gateway/main/docs/samples/sample_azure_config_sp.yaml
# config:
#   azureCloudConfig:
#     cloud: "AzurePublicCloud"
#     tenantId: "<tenant ID>"
#     subscriptionId: "<subscription ID>"
#     useManagedIdentityExtension: false
#     aadClientId: "<sp clientID>"
#     aadClientSecret: "<sp secret>"
#     userAgent: "kube-egress-gateway-controller"
#     resourceGroup: "<resource group name>"
#     location: "<resource group location>"
#     gatewayLoadBalancerName: "kubeegressgateway-ilb"
#     loadBalancerResourceGroup: ""
#     vnetName: "<virtual network name>"
#     vnetResourceGroup: ""
#     subnetName: "<subnet name>"
app_name=egressgwsp
rg_id=$(az group show -n $rg -o tsv --query id)
sp_output=$(az ad sp create-for-rbac -n $app_name --role Contributor --scopes $rg_id)
sp_appid=$(echo $sp_output | jq -r '.appId')
sp_password=$(echo $sp_output | jq -r '.password')
wget $url -q -O $config_file
tenant_id=$(az account show -o tsv --query tenantId)
subscription_id=$(az account show --query id -o tsv)
id_id=$(az identity show -g $rg -n $id_name -o tsv --query id)
sed -i "s|<sp clientID>|${sp_appid}|g" $config_file
sed -i "s|<sp secret>|${sp_password}|g" $config_file
sed -i "s|<tenant ID>|${tenant_id}|g" $config_file
sed -i "s|<subscription ID>|${subscription_id}|g" $config_file
sed -i "s|<resource group name>|${rg}|g" $config_file
sed -i "s|<virtual network name>|${vnet_name}|g" $config_file
sed -i "s|<subnet name>|${aks_subnet_name}|g" $config_file
sed -i "s|<resource group location>|${location}|g" $config_file


# Helm chart install
git clone https://github.com/Azure/kube-egress-gateway.git
helm delete -n kube-egress-gateway-system kube-egress-gateway     # In case you are re-installing the helm chart
helm install kube-egress-gateway ./kube-egress-gateway/helm/kube-egress-gateway \
  --namespace kube-egress-gateway-system \
  --create-namespace \
  --set "common.imageRepository=mcr.microsoft.com/aks" --set "common.imageTag=v0.0.1" \
  --values "$config_file"

# Deploy Gateway config
# https://github.com/azure/kube-egress-gateway
kubectl apply -f - <<EOF
apiVersion: egressgateway.kubernetes.azure.com/v1alpha1
kind: StaticGatewayConfiguration
metadata:
  name: mystaticegressgateway
  namespace: default
spec:
  gatewayVmssProfile:
    vmssResourceGroup: $rg
    vmssName: myGatewayPool
    publicIpPrefixSize: 31
  provisionPublicIps: true
EOF

# Create pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: yadaapi
  labels:
    app: yadaapi
  annotations:
    kubernetes.azure.com/static-gateway-configuration: mystaticegressgateway
spec:
  containers:
  - name: yadaapi
    image: erjosito/yadaapi:1.0
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: yadapi
spec:
  type: LoadBalancer
  ports:
  - port: 8080
  selector:
    app: yada
EOF


#####################
#       Istio       #
#####################

# Seems not to work wiht Cilium

# Enable Istio
echo "Enabling Istio addon..."
az aks mesh enable -n $aks_name -g $rg -o none
kubectl get pods -n aks-istio-system

# Enable ingress gateway
az aks mesh enable-ingress-gateway -n $aks_name -g $rg --ingress-gateway-type external -o none
kubectl get svc -n aks-istio-ingress

# Disable ingress gateway
az aks mesh disable-ingress-gateway -n $aks_name -g $rg --ingress-gateway-type external -y

# Deploy Gateway
# https://learn.microsoft.com/en-us/azure/aks/istio-deploy-ingress
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: api-gateway-external
spec:
  selector:
    istio: aks-istio-ingressgateway-external
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
EOF
# Virtual Service
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: api-external
spec:
  hosts:
  - "*"
  gateways:
  - api-gateway-external
  http:
  - match:
    - uri:
        prefix: /api/
    route:
    - destination:
        host: yadaapi
        port:
          number: 80
EOF

# https://istio.io/latest/docs/tasks/traffic-management/request-routing/
kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: yadaapi
spec:
  parentRefs:
  - group: ""
    kind: Service
    name: reviews
    port: 8080
  rules:
  - backendRefs:
    - name: reviews-v1
      port: 9080
EOF

# TCP routing
# ===========

# Deploy TCP echo service
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tcp-echo-deployment
  labels:
    app: tcp-echo
    system: example
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tcp-echo
  template:
    metadata:
      labels:
        app: tcp-echo
        system: example
    spec:
      containers:
        - name: tcp-echo-container
          image: cjimti/go-echo:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: TCP_PORT
              value: "2701"
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
          ports:
            - name: tcp-echo-port
              containerPort: 2701
---
apiVersion: v1
kind: Service
metadata:
  name: "tcp-echo-service"
  labels:
    app: tcp-echo
    system: example
spec:
  selector:
    app: "tcp-echo"
  ports:
    - protocol: "TCP"
      port: 2701
      targetPort: 2701
EOF
# Gateway
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: echo-tcp-gateway
spec:
  selector:
    istio: aks-istio-ingressgateway-external
  servers:
  - port:
      number: 31400
      name: tcp-echo
      protocol: TCP
    hosts:
    - "*"
EOF
# Virtual Service
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: tcp-echo-vs-from-gw
spec:
  hosts:
  - "*"
  gateways:
  - echo-tcp-gateway
  tcp:
  - match:
    - port: 31400
    route:
    - destination:
        host: tcp-echo-service
        port:
          number: 2701
EOF

# Port 31400 had to be manually added to the SVC in aks-istio-ingress!!
  # - name: tcpecho
  #   port: 31400
  #   protocol: TCP
  #   targetPort: 31400
# (the nodeport will be added automatically)

# From https://istio.io/latest/docs/tasks/traffic-management/request-routing/
# Not working with the AKS addon!
kubectl get crd gateways.gateway.networking.k8s.io &> /dev/null || \
  { kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd?ref=v0.8.0" | kubectl apply -f -; }
kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd/experimental?ref=v0.8.0" | kubectl apply -f 
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.19/samples/tcp-echo/tcp-echo-services.yaml
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.19/samples/tcp-echo/gateway-api/tcp-echo-all-v1.yaml

#####################
# Create Az Bastion #
#####################

echo "Creating Azure Bastion..."
bastion_pip_name="${bastion_name}-pip"
az network vnet subnet create -g $rg -n $bastion_subnet_name --vnet-name $vnet_name --address-prefix $bastion_subnet_prefix -o none
az network public-ip create -g $rg -n $bastion_pip_name --sku Standard -l $location -o none
az network bastion create -n $bastion_name --public-ip-address $bastion_pip_name -g $rg --vnet-name $vnet_name -l $location \
    --enable-ip-connect true --enable-tunneling true  -o none
vm_id=$(az vm show -n $vm_name -g $rg --query id -o tsv)
az vm user update -u azureuser -p 'Microsoft123!' -n $vm_name -g $rg -o none
az network bastion tunnel -n $bastion_name -g $rg --target-resource-id $vm_id --resource-port 22 --port 2022

#####################
#  IP restrictions  #
#####################

myip=$(curl -s4 ifconfig.co) && echo $myip
az aks update -g $rg -n $aks_name --api-server-authorized-ip-ranges "${myip}/32"

#####################
# Add a second pool #
#####################

az network vnet subnet create -g $rg -n $aks2_subnet_name --vnet-name $vnet_name --address-prefix $aks2_subnet_prefix -o none
az network vnet subnet create -g $rg -n $aks2_pod_subnet_name --vnet-name $vnet_name --address-prefix $aks2_pod_subnet_prefix -o none
# vm_size=Standard_DS3_v2
vm_size=Standard_B2ms
aks2_subnet_id=$(az network vnet subnet show -n $aks2_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
aks2_pod_subnet_id=$(az network vnet subnet show -n $aks2_pod_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
az aks nodepool add --cluster-name $aks_name -g $rg -n pool2 --node-count 1 -s $vm_size \
    -k $k8s_version --mode User --vnet-subnet-id $aks2_subnet_id \
    --pod-subnet-id $aks2_pod_subnet_id
    # --node-osdisk-type Ephemeral
    # --enable-cluster-autoscaler --min-count 1 --max-count 2 \
az aks nodepool list --cluster-name $aks_name -g $rg -o table

###########################
# Deploy sample workloads #
###########################

if [[ "$aks_isolated" == "yes" ]]; then
    echo "INFO: Cluster is isolated, creating cache in ACR $acr_name..."
    # Create AKV
    akv_name=$(az keyvault list -g $rg --query '[0].name' -o tsv)
    if [[ -z "$akv_name" ]]; then
        akv_name=$acr_name
        echo "INFO: Creating Key Vault $akv_name..."
        az keyvault create -n $akv_name -g $rg -l $location --sku standard -o none
        akv_id=$(az keyvault show -n $akv_name -g $rg --query id -o tsv)
        # Diagnostic settings for AKV
        az monitor diagnostic-settings create -n diag$RANDOM --resource $akv_id --workspace $logws_id -o none \
            --logs '[{"category": "AuditEvent", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}]'
        # Private endpoint for AKV
        akv_ep_name="${akv_name}-akv-ep"
        az network private-endpoint create -n $akv_ep_name -g $rg --vnet-name $vnet_name --subnet $ep_subnet_name \
            --private-connection-resource-id $akv_id --group-ids vault --connection-name "${akv_ep_name}-conn" -o none
        akv_ep_id=$(az network private-endpoint show -n $akv_ep_name -g $rg --query id -o tsv)
        akv_zone_name="privatelink.vaultcore.azure.net"
        az network private-dns zone create -g $rg -n "$akv_zone_name" -o none
        az network private-dns link vnet create -g $rg -n "$akv_zone_name" -z "$akv_zone_name" \
            --virtual-network $vnet_name --registration-enabled false -o none
        az network private-endpoint dns-zone-group create -g $rg -n "${akv_ep_name}-dnszonegroup" --endpoint-name $akv_ep_name \
            --private-dns-zone "$akv_zone_name" --zone-name "vault" -o none
        # Identity-based network access working with AKV :)
        # az keyvault update -n $akv_name --public-network-access Enabled -o none # Troubleshooting
        # Create identity for ACR to access AKV
        # User identity not supported yet: https://github.com/Azure/acr/issues/790?ref=dariuszparys.com
        # acr_id_name="${acr_name}-id"
        # acr_id_id=$(az identity show -g $rg -n $acr_id_name --query id -o tsv 2>/dev/null)
        # if [[ -z "$acr_id_id" ]]; then
        #     echo "INFO: Creating identity $acr_id_name for ACR to access AKV..."
        #     acr_id_id=$(az identity create -g $rg -n $acr_id_name --query id -o tsv)
        #     az acr identity assign -n $acr_name -g $rg --identities $acr_id_id -o none
        # else
        #     echo "INFO: Using existing identity $acr_id_name for ACR to access AKV..."
        # fi
        # acr_id_principal_id=$(az identity show -g $rg -n $acr_id_name --query principalId -o tsv)
        # Managed identity
        az acr identity assign --identities '[system]' -n $acr_name -o none
        acr_id_principal_id=$(az acr identity show -n $acr_name --query principalId -o tsv)
        # Assign RBAC role to the identity
        az role assignment create --assignee $acr_id_principal_id --role "Key Vault Secrets User" --scope $akv_id -o none
        current_user_principal_id=$(az ad signed-in-user show --query id -o tsv)
        az role assignment create --assignee $current_user_principal_id --role "Key Vault Secrets Officer" --scope $akv_id -o none
        # Add secrets to AKV
        echo "INFO: Adding secrets to Key Vault $akv_name..."
        az keyvault secret set --vault-name $akv_name --name dockerusername --value $docker_username -o none
        az keyvault secret set --vault-name $akv_name --name dockerpat --value $docker_pat -o none
    else
        echo "INFO: Using existing Key Vault $akv_name..."
    fi
    # Create ACR credential set
    az acr credential-set create -r $acr_name -n dockerhub -l docker.io -u "https://${akv_name}.vault.azure.net/secrets/dockerusername" -p "https://${akv_name}.vault.azure.net/secrets/dockerpat" -o none
    # Apparently the credset doesnt use ACR's own system managed identity, but its own (!!!!)
    credset_principal_id=$(az acr credential-set show -n dockerhub -r $acr_name --query identity.principalId -o tsv)
    az role assignment create --assignee $credset_principal_id --role "Key Vault Secrets User" --scope $akv_id -o none
    sleep 15  # Wait for role assignment to propagate
    # Create ACR cache for yadaapi image
    az acr cache create -r $acr_name -n yadaapi --source-repo docker.io/erjosito/yadaapi --target-repo yadaapi --cred-set dockerhub -o none
    # Set image variable to point to ACR
    yada_image="${acr_name}.azurecr.io/yadaapi:1.0"
else
    yada_image="erjosito/yadaapi:1.0"
fi

az aks get-credentials -n $aks_name -g $rg --overwrite
kubectl create deployment kuard --image=gcr.io/kuar-demo/kuard-amd64:blue --port=8080 --replicas=1
kubectl create deployment yadaapi --image=$yada_image --port=8080 --replicas=1
# kubectl run yadaapi --image=erjosito/yadaapi:1.0 --port=8080 --env="SQL_SERVER_FQDN=${sql_server_fqdn}" --env="SQL_SERVER_USERNAME=${sql_username}" --env="SQL_SERVER_PASSWORD=${sql_password}"

# Full YADA app
sql_server_name=sqlserver$RANDOM
sql_db_name=mydb
sql_username=azure
sql_password=$(openssl rand -base64 10)  # 10-character random password
echo "Creating SQL Server $sql_server_name..."
az sql server create -n $sql_server_name -g $rg -l $location --admin-user "$sql_username" --admin-password "$sql_password" -o none
az sql db create -n $sql_db_name -s $sql_server_name -g $rg -e Basic -c 5 --no-wait -o none
sql_server_fqdn=$(az sql server show -n $sql_server_name -g $rg -o tsv --query fullyQualifiedDomainName)
echo "SQL server created: $sql_server_fqdn"
echo "Creating API..."
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: sqlpassword
type: Opaque
stringData:
  password: $sql_password
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: api
  name: api
spec:
  replicas: 1
  selector:
    matchLabels:
      run: api
  template:
    metadata:
      labels:
        run: api
    spec:
      containers:
      - image: erjosito/yadaapi:1.0
        name: api
        ports:
        - containerPort: 8080
          protocol: TCP
        env:
        - name: SQL_SERVER_USERNAME
          value: "$sql_username"
        - name: SQL_SERVER_FQDN
          value: "$sql_server_fqdn"
        - name: SQL_SERVER_PASSWORD
          valueFrom:
            secretKeyRef:
              name: sqlpassword
              key: password
      restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: api
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 8080
  selector:
    run: api
EOF
echo "Creating Web..."
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: web
  name: web
spec:
  replicas: 1
  selector:
    matchLabels:
      run: web
  template:
    metadata:
      labels:
        run: web
    spec:
      containers:
      - image: erjosito/yadaweb:1.0
        name: web
        ports:
        - containerPort: 80
          protocol: TCP
        env:
        - name: API_URL
          value: "http://api:8080"
      restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
  selector:
    run: web
EOF
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-netpol
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: api
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              run: web
        - ipBlock:
            cidr: 10.13.76.0/24
        - ipBlock:
            cidr: 168.63.129.16/32
      ports:
        - protocol: TCP
          port: 8080
  egress:
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: kube-system
          podSelector:
            matchLabels:
              k8s-app: kube-dns
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 1433
        - protocol: UDP
          port: 53
EOF
cat <<EOF | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: api-netpol
  namespace: default
spec:
  endpointSelector:
    matchLabels:
      run: api
  ingress:
  - fromEndpoints:
    - matchLabels:
        run: web
    toPorts:
      - ports:
          - port: "8081"
            protocol: "TCP"
  egress:
    - toEndpoints:
      - matchLabels:
          "k8s:io.kubernetes.pod.namespace": kube-system
          "k8s:k8s-app": kube-dns
      toPorts:
        - ports:
           - port: "53"
             protocol: ANY
EOF
yadaapi_ip=$(kubectl get svc/api -o json | jq -rc '.status.loadBalancer.ingress[0].ip' 2>/dev/null) && echo $yadaapi_ip
yadaweb_ip=$(kubectl get svc/web -o json | jq -rc '.status.loadBalancer.ingress[0].ip' 2>/dev/null) && echo $yadaweb_ip
curl -s4 "http://${yadaapi_ip}:8080/api/healthcheck"
curl -s4 "http://${yadaweb_ip}/healthcheck.html"
yadaapi_egress_ip=$(curl -s "http://${yadaapi_ip}:8080/api/ip" | jq -r .my_public_ip)
echo "YADA API egress IP: $yadaapi_egress_ip"
az sql server firewall-rule create -g "$rg" -s "$sql_server_name" -n public_api_aci-source --start-ip-address "$yadaapi_egress_ip" --end-ip-address "$yadaapi_egress_ip"


# Service with kubectl
kubectl expose deploy yadaapi --port=80 --target-port=8080

# Service
app_name=yadaapi
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-internal: "true"
    # service.beta.kubernetes.io/azure-load-balancer-internal-subnet: "akslb"
  name: $app_name
spec:
  type: ClusterIP
  ports:
  - port: 8080
  selector:
    app: $app_name
EOF

# Service with Static PIP
svc_pip_name=k8ssvcpip
az network public-ip create -g $rg -n $svc_pip_name --sku Standard --allocation-method static
svc_pip_ip=$(az network public-ip show -n $svc_pip_name -g $rg --query ipAddress -o tsv) && echo $svc_pip_ip
svc_pip_id=$(az network public-ip show -n $svc_pip_name -g $rg --query id -o tsv) && echo $svc_pip_id
if [[ "$use_msi" == no ]]
then
    client_id=$sp_app_id
else
    client_id=$id_principal_id
fi
az role assignment create --assignee $client_id --role "Network Contributor" --scope $svc_pip_id
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-resource-group: $rg
  name: kuard
spec:
  loadBalancerIP: $svc_pip_ip
  type: LoadBalancer
  ports:
  - port: 8080
  selector:
    app: kuard
EOF

# Workload with PV/PVC (disk performance test)
yaml_file=/tmp/pvc.yaml
cat <<EOF > $yaml_file
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: premiumpvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: managed-premium
  resources:
    requests:
      storage: 5Gi
---
kind: Pod
apiVersion: v1
metadata:
  name: sqlapi
  labels:
    run: sqlapi
spec:
  containers:
  - name: mypod
    image: erjosito/sqlapi:1.0
    imagePullPolicy: Always
    resources:
      requests:
        cpu: 250m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 256Mi
    volumeMounts:
    - mountPath: "/mnt/azure"
      name: volume
  volumes:
    - name: volume
      persistentVolumeClaim:
        claimName: premiumpvc
---
apiVersion: v1
kind: Service
metadata:
  labels:
    run: sqlapi
  name: sqlapi
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    run: sqlapi
  sessionAffinity: None
  type: LoadBalancer
EOF
kubectl apply -f $yaml_file
# Diagnostics
k get sc
k get pvc
k get pv
k get svc
node1_name=$(k get node -o jsonpath='{.items[0].metadata.name}') && echo $node1_name
node1_drivers=$(kubectl get CSINode $node1_name -o jsonpath="{.spec.drivers}") && echo $node1_drivers
node1_disk_count=$(kubectl get CSINode $node1_name -o jsonpath="{.spec.drivers[1].allocatable.count}") && echo $node1_disk_count
svc_ip=$(kubectl get svc/sqlapi -o json | jq -rc '.status.loadBalancer.ingress[0].ip' 2>/dev/null) && echo $svc_ip
curl "http://${svc_ip}/api/healthcheck"
curl "http://${svc_ip}/api/healthcheck/api/ioperf?file=%2Fmnt%2Fazure%2Fiotest"

########################
# NFS Azure file share #
########################

# Create file share
file_share_name=myshare$RANDOM
az resource create \
  --resource-type "Microsoft.FileShares/fileShares" \
  --name $file_share_name \
  --location $location \
  --resource-group $rg \
  --properties "{
    \"redundancy\": \"Local\",
    \"protocol\": \"NFS\",
    \"provisionedStorageGiB\": 32,
    \"ProvisionedIoPerSec\": 3032,
    \"ProvisionedThroughputMiBPerSec\": 128,
    \"mediaTier\": \"SSD\",
    \"nfsProtocolProperties\": {
      \"rootSquash\": \"RootSquash\"
    }
}" -o none
# Create private endpoint
vnet_name=aksVNet
subnet_name=ep
subnet_id=$(az network vnet subnet show -n $subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
file_share_id=$(az resource show -g $rg -n $file_share_name --resource-type "Microsoft.FileShares/fileShares" --query id -o tsv)
ep_name="${file_share_name}-ep"
az network private-endpoint create -n $ep_name -g $rg --vnet-name $vnet_name --subnet $subnet_name \
  --private-connection-resource-id $file_share_id --group-ids FileShare --connection-name "${ep_name}-conn" -o none
# Create DNS zone and records
dns_zone_name="privatelink.file.core.windows.net"
az network private-dns zone create -g $rg -n $dns_zone_name -o none
az network private-dns link vnet create -g $rg -n "${vnet_name}-link" --virtual-network $vnet_name --zone-name $dns_zone_name --registration-enabled false -o none
az network private-endpoint dns-zone-group create -g $rg -n "${ep_name}-dnszonegroup" --endpoint-name $ep_name --private-dns-zone $dns_zone_name --zone-name file -o none
# Check the A recors in the DNS zone
az network private-dns record-set a list -z $dns_zone_name -g $rg --query '[].{IP:aRecords[0].ipv4Address, FQDN:fqdn, Creator:metadata.creator}' -o table
# Output instructions
file_share_hostname=$(az resource show -g $rg -n $file_share_name --resource-type "Microsoft.FileShares/fileShares" --query properties.hostName -o tsv)
storage_account_name=$(echo $file_share_hostname | cut -d'.' -f1)
echo "File share created: $file_share_name"
echo "To mount the file share in a VM, use the following command:"
echo "sudo mount -t nfs ${file_share_hostname}:/${storage_account_name}/${file_share_name} /mnt/azure -o vers=4,minorversion=1,sec=sys"
# Create k8s resources
cat <<EOF | kubectl apply -f -
kind: Pod
apiVersion: v1
metadata:
  name: sqlapi
  labels:
    run: sqlapi
spec:
  containers:
  - name: mypod
    image: yada/sqlapi:1.0
    imagePullPolicy: Always
    resources:
      requests:
        cpu: 250m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 256Mi
    volumeMounts:
    - mountPath: "/mnt/azure"
      name: nfs
  volumes:
  - name: nfs
    nfs:
      server: $file_share_hostname
      path: "/$storage_account_name/$file_share_name"
EOF

###########################
#          AGIC           #
###########################

# Create AGIC addon
echo "Creating subnet for AppGW..."
az network vnet subnet create -n $appgw_subnet_name --vnet-name $vnet_name -g $rg --address-prefixes $appgw_subnet_prefix -o none
appgw_subnet_id=$(az network vnet subnet show -n $appgw_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
echo "Enabling AGIC..."
az aks enable-addons -a ingress-appgw -n $aks_name -g $rg --appgw-subnet-id $appgw_subnet_id -o none

# Create ingress rule
node_rg=$(az aks show -n $aks_name -g $rg --query nodeResourceGroup -o tsv)
appgw_name=$(az network application-gateway list -g $node_rg --query '[0].name' -o tsv)
appgw_pip_id=$(az network application-gateway frontend-ip list --gateway-name $appgw_name -g $node_rg --query '[0].publicIpAddress.id' -o tsv)
appgw_pip=$(az network public-ip show --ids $appgw_pip_id --query 'ipAddress' -o tsv)
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: sqlapi
  annotations:
    kubernetes.io/ingress.class: azure/application-gateway
    appgw.ingress.kubernetes.io/health-probe-path: "/api/healthcheck"
spec:
  rules:
  - host: sqlapi.${appgw_pip}.nip.io
    http:
      paths:
      - path: /api/
        pathType: Prefix
        backend:
          service:
            name: sqlapi
            port:
              number: 8080
EOF

curl "http://sqlapi.${appgw_pip}.nip.io/api/healthcheck"
curl "http://sqlapi.${appgw_pip}.nip.io/api/ip"

##################
#  Private Link  #
##################

# Create subnet
az network vnet subnet create -n $ep_subnet_name --vnet-name $vnet_name -g $rg --address-prefixes $ep_subnet_prefix -o none

# Create SQL Database
sql_server_name=sqlserver$RANDOM
sql_db_name=mydb
sql_username=azure
sql_password='Microsoft123!'
az sql server create -n $sql_server_name -g $rg -l $location --admin-user $sql_username --admin-password $sql_password -o none
sql_server_fqdn=$(az sql server show -n $sql_server_name -g $rg -o tsv --query fullyQualifiedDomainName)
az sql db create -n $sql_db_name -s $sql_server_name -g $rg -e Basic -c 5 --no-wait

# Create endpoint
echo "Creating SQL Server endpoint..."
sql_endpoint_name=sqlep
sql_server_id=$(az sql server show -n $sql_server_name -g $rg -o tsv --query id)
az network private-endpoint create -n $sql_endpoint_name -g $rg -o none\
  --vnet-name $vnet_name --subnet $ep_subnet_name \
  --private-connection-resource-id $sql_server_id --group-id sqlServer --connection-name sqlConnection

# Create Azure DNS private zone and records
dns_zone_name=privatelink.database.windows.net
echo "Creating DNS zone $dns_zone_name..."
az network private-dns zone create -n $dns_zone_name -g $rg -o none
az network private-dns link vnet create -g $rg -z $dns_zone_name -n myDnsLink --virtual-network $vnet_name --registration-enabled false -o none
az network private-endpoint dns-zone-group create --endpoint-name $sql_endpoint_name -g $rg -n sqlzonegroup --zone-name zone1 --private-dns-zone $dns_zone_name -o none

# Create a deployment:
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: sqlpassword
type: Opaque
stringData:
  password: $sql_password
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: sqlapi
  name: sqlapi
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sqlapi
  template:
    metadata:
      labels:
        app: sqlapi
    spec:
      containers:
      - image: erjosito/sqlapi:1.0
        name: sqlapi
        ports:
        - containerPort: 8080
          protocol: TCP
        env:
        - name: SQL_SERVER_USERNAME
          value: "$sql_username"
        - name: SQL_SERVER_FQDN
          value: "$sql_server_fqdn"
        - name: SQL_SERVER_PASSWORD
          valueFrom:
            secretKeyRef:
              name: sqlpassword
              key: password
      restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: sqlapi
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
  selector:
    app: sqlapi
EOF

# Test
curl "http://sqlapi.${appgw_pip}.nip.io/api/dns?fqdn=${sql_server_fqdn}"
curl "http://sqlapi.${appgw_pip}.nip.io/api/sqlsrcip"


##############################
# Daemonset to install stuff #
##############################

kubectl create ns node-installer
yaml_file=/tmp/podid.yaml
cat <<EOF > $yaml_file
apiVersion: v1
kind: ConfigMap
metadata:
  name: sample-installer-config
  namespace: node-installer
data:
  install.sh: |
    #!/bin/bash
    # echo "Updating repositories..."
    # sudo touch /var/lib/man-db/auto-update
    # apt-get update
    # echo "Installing sample app..."
    # apt-get install cowsay -y
    echo "Restarting containerd..."
    # service containerd restart
    systemctl restart containerd
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: installer
  namespace: node-installer
spec:
  selector:
    matchLabels:
      job: installer
  template:
    metadata:
      labels:
        job: installer
    spec:
      hostPID: true
      restartPolicy: Always
      containers:
      - image: patnaikshekhar/node-installer:1.3
        name: installer
        securityContext:
          privileged: true
        volumeMounts:
        - name: install-script
          mountPath: /tmp
        - name: host-mount
          mountPath: /host
      volumes:
      - name: install-script
        configMap:
          name: sample-installer-config
      - name: host-mount
        hostPath:
          path: /tmp/install
EOF
kubectl apply -f $yaml_file

#####################
#   Azure Advisor   #
#####################

aks_id=$(az aks show -n $aks_name -g $rg --query id -o tsv)
az advisor recommendation list --ids $aks_id -o table

#######################
#  Workload Identity  #
#######################

# Variables
workload_id_name=workloadidtest   # Name of the User-Assigned Managed Identity
federated_id_name=federatedid     # Name for the identity federation
akv_name=erjositoKeyvault         # Keyvault to test
akv_secret_name=defaultPassword       # Secret to test
sa_name=myserviceaccount
sa_namespace=default

# Enable workload identity and create identity
enableAksFeature EnableWorkloadIdentityPreview
echo "Enabling workload identity in cluster..."
az aks update -n $aks_name -g $rg --enable-oidc-issuer --enable-workload-identity true -o none
aks_oidc_issuer="$(az aks show -n $aks_name -g $rg --query "oidcIssuerProfile.issuerUrl" -o tsv)"
echo "Creating managed identity..."
az identity create -n $workload_id_name -g $rg -o none
workload_id_id="$(az identity show -n $workload_id_name -g $rg --query 'clientId' -o tsv)"
sleep 30  # Sometimes it takes some time to properly create the identity
az keyvault set-policy -n $akv_name --secret-permissions get --spn "${workload_id_id}" -o none

# Create service account
echo "Creating service account in AKS cluster..."
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    azure.workload.identity/client-id: ${workload_id_id}
  labels:
    azure.workload.identity/use: "true"
  name: $sa_name
  namespace: $sa_workspace
EOF

# Federate identities
echo "Federating managed identity and service account..."
az identity federated-credential create -n $federated_id_name --identity-name $workload_id_name -g $rg \
    --issuer ${aks_oidc_issuer} --subject "system:serviceaccount:${sa_namespace}:${sa_name}" -o none

# Mutating webhook (not required if using addon)
# tenant_id=$(az account show -o tsv --query tenantId)
# echo "Creating mutating webhook for tenant $tenant_id and service account $sa_name..."
# helm repo add azure-workload-identity https://azure.github.io/azure-workload-identity/charts
# helm repo update
# helm install workload-identity-webhook azure-workload-identity/workload-identity-webhook --namespace azure-workload-identity-system \
#   --create-namespace --set azureTenantID="${tenant_id}"

# Test (deploy a workload, and get an AKV secret from the workload)
# Deploy a database with our default password
echo "Creating Azure SQL..."
sql_server_name=sqlserver$RANDOM
sql_db_name=mydb
sql_username=azure
sql_password=$(az keyvault secret show --vault-name $akv_name -n $akv_secret_name --query value -o tsv)
az sql server create -n $sql_server_name -g $rg -l $location --admin-user "$sql_username" --admin-password "$sql_password" -o none
az sql db create -n $sql_db_name -s $sql_server_name -g $rg -e Basic -c 5 --no-wait -o none
sql_server_fqdn=$(az sql server show -n $sql_server_name -g $rg -o tsv --query fullyQualifiedDomainName) && echo $sql_server_fqdn
# Note the annotation and the serviceAccount name in the spec
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: yadaapi
  labels:
    # azure.workload.identity/use: "true"
    app: yadaapi
spec:
  serviceAccountName: ${sa_name}
  containers:
    - image: erjosito/yadaapi:1.0
      name: yadaapi
      env:
      - name: SQL_SERVER_FQDN
        value: ${sql_server_fqdn}
      - name: SQL_SERVER_USERNAME
        value: ${sql_username}
      - name: AKV_NAME
        value: ${akv_name}
      - name: AKV_SECRET_NAME
        value: ${akv_secret_name}
  nodeSelector:
    kubernetes.io/os: linux
EOF
# Create a svc and get the IP
kubectl expose deploy yadaapi --port=80 --target-port=8080 --type=LoadBalancer
sleep 60
yada_svc_ip=$(kubectl get svc/yadaapi -o json | jq -rc '.status.loadBalancer.ingress[0].ip' 2>/dev/null) && echo $yada_svc_ip
curl "http://${yada_svc_ip}/api/healthcheck"
# Get the public IP to allow it in the SQL DB firewall
api_outbound_pip=$(curl -s "http://${yada_svc_ip}/api/ip" | jq -r .my_public_ip)
echo "Adding $api_outbound_pip to Azure SQL firewall rules..."
az sql server firewall-rule create -g $rg -s $sql_server_name -n yadaapi --start-ip-address $api_outbound_pip --end-ip-address $api_outbound_pip -o none
# Get the secret name from AKV
curl "http://${yada_svc_ip}/api/akvsecret?akvname=${akv_name}&akvsecret=${akv_secret_name}"
# Get the SQL Version (involves getting the secret name from AKV)
curl "http://${yada_svc_ip}/api/sqlversion"

# Troubleshoot
kubectl get MutatingWebhookConfiguration
kubectl describe MutatingWebhookConfiguration/azure-wi-webhook-mutating-webhook-configuration
kubectl get config-map -A
pod_name=$(kubectl get pod -l app=yadaapi -o json | jq -r '.items[0].metadata.name') && echo $pod_name
kubectl describe pod $pod_name
kubectl get pod yadaapi -o json | jq -r '.spec.containers[0].env'


#######################
# Pod Identity - Helm #
#######################

# Pod identity
id_name=apppodid
az identity create --resource-group $rg --name $id_name
id_client_id="$(az identity show -g $rg -n $id_name --query clientId -o tsv)" && echo $id_client_id
id_arm_id="$(az identity show -g $rg -n $id_name --query id -o tsv)" && echo $id_arm_id
az aks update -n $aks_name -g $rg --enable-pod-identity
podid_ns=default
az aks pod-identity add -g $rg --cluster-name $aks_name --namespace $podid_ns --name $id_name --identity-resource-id $id_arm_id
node_rg=$(az aks show -n $aks_name -g $rg --query nodeResourceGroup -o tsv) && echo $node_rg
node_rg_id=$(az group show -n $node_rg --query id -o tsv) && echo $node_rg_id
rg_id=$(az group show -n $rg --query id -o tsv) && echo $rg_id
az role assignment create --role "Reader" --assignee "$id_client_id" --scope $node_rg_id
az role assignment create --role "Reader" --assignee "$id_client_id" --scope $rg_id
subscription_id=$(az account show --query id -o tsv) && echo $subscription_id
yaml_file=/tmp/podid.yaml
cat <<EOF > $yaml_file
apiVersion: v1
kind: Pod
metadata:
  name: demo
  labels:
    aadpodidbinding: $id_name
spec:
  containers:
  - name: demo
    image: mcr.microsoft.com/oss/azure/aad-pod-identity/demo:v1.6.3
    args:
      - --subscriptionid=$subscription_id
      - --clientid=$id_client_id
      - --resourcegroup=$rg
    env:
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: MY_POD_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: MY_POD_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
  nodeSelector:
    kubernetes.io/os: linux
EOF
kubectl apply -f $yaml_file --namespace $podid_ns
kubectl logs demo --follow --namespace $podid_ns
az aks pod-identity list --cluster-name $aks_name -g $rg -o table
az aks pod-identity exception list --cluster-name $aks_name -g $rg -o table

#########
# kured #
#########

helm repo add kured https://weaveworks.github.io/kured
helm repo update
kubectl create namespace kured
helm install kured kured/kured --namespace kured --set nodeSelector."beta\.kubernetes\.io/os"=linux

#######################
# Jump host / test VM #
#######################

vm_name=testvm
vm_pip_name=testvm-pip
vm_sku=Standard_B1s
echo "Creating subnet..."
echo "Creating VM..."
az vm create -n $vm_name -g $rg --image UbuntuLTS --generate-ssh-keys --size $vm_sku -l $location \
   --vnet-name $vnet_name --subnet $vm_subnet_name --public-ip-address $vm_pip_name --public-ip-sku Standard -o none
vm_pip_address=$(az network public-ip show -n $vm_pip_name -g $rg --query ipAddress -o tsv) && echo $vm_pip_address
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip_address "ip a"
node_ip=$(kubectl get node -o json | jq -r '.items[0].status.addresses[] | select(.type=="InternalIP") | .address') && echo $node_ip
# Connect to an AKS node over the jump VM
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no -m hmac-sha2-512 -J $vm_pip_address $node_ip "ip a"
# Install Azure CLI and kubectl on the jump VM
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip_address "sudo apt update"
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip_address "curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash"
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vm_pip_address "sudo az aks install-cli"

#######################
# OpenVPN Server test #
#######################

# https://github.com/rishasi/OpenVPN-on-AKS

# Server pod
helm repo add stenic https://stenic.github.io/helm-charts
helm install openvpnserver --set "service.type=LoadBalancer" --set "service.gui.type=LoadBalancer" --set "service.admin.type=LoadBalancer"
k get svc

# Open iptables in the server - Is this really required???
openvpn_server_pod=$(k get pod -l app.kubernetes.io/instance=openvpnserver -o json | jq -r '.items[0].metadata.name') && echo $openvpn_server_pod
kubectl exec $openvpn_server_pod -- iptables -A INPUT -i as0t+ -j ACCEPT
kubectl exec $openvpn_server_pod -- iptables -A FORWARD -i as0t+ -j ACCEPT

# default credentials: altmin/passw0rd -> Download client profile from the 9443 svc to current directory
pip_943_tcp=$(kubectl get service openvpnserver-openvpn-as-admin -o json | jq -r '.status.loadBalancer.ingress[0].ip')
echo "Connect to http://${pip_943_tcp}:943 to download the client profile"
client_ovpn_file=./client.ovpn

# Client file for VM (using public IPs)
# Replace in the file the OpenVPN Server Pod IP with the Service IP (ALB)
clientvm_ovpn_file=./clientvm.ovpn
cp $client_ovpn_file $clientvm_ovpn_file
pip_1194_udp=$(kubectl get service openvpnserver-openvpn-as-udp -o json | jq -r '.status.loadBalancer.ingress[0].ip') && echo $pip_1194_udp
pip_9443_tcp=$(kubectl get service openvpnserver-openvpn-as-tcp -o json | jq -r '.status.loadBalancer.ingress[0].ip') && echo $pip_9443_tcp
private_ip=$(k get pod -l app.kubernetes.io/instance=openvpnserver -o json | jq -r '.items[0].status.podIP') && echo $private_ip
sed -i "s/$private_ip 1194 udp/$pip_1194_udp 1194 udp/g" $clientvm_ovpn_file
sed -i "s/$private_ip 9443 tcp/$pip_9443_tcp 9443 tcp/g" $clientvm_ovpn_file

# Client file for VM (using cluster IPs)
clientpod_ovpn_file=./clientpod.ovpn
cp $client_ovpn_file $clientpod_ovpn_file
clusterip_1194_udp=$(kubectl get service openvpnserver-openvpn-as-udp -o json | jq -r '.spec.clusterIP') && echo $clusterip_1194_udp
clusterip_9443_tcp=$(kubectl get service openvpnserver-openvpn-as-tcp -o json | jq -r '.spec.clusterIP') && echo $clusterip_9443_tcp
sed -i "s/$private_ip 1194 udp/$clusterip_1194_udp 1194 udp/g" $clientpod_ovpn_file
sed -i "s/$private_ip 9443 tcp/$clusterip_9443_tcp 9443 tcp/g" $clientpod_ovpn_file

# Client pod
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: openvpn-client
  name: openvpn-client
spec:
  containers:
  - image: debian
    name: openvpn-client
    command:
      - sleep
      - "36000"
    securityContext:
      capabilities:
        add: ["NET_ADMIN"]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
EOF

# Copy client file and generate credentials file
credentials_file=/tmp/login.conf
cat <<EOF > $credentials_file
altmin
passw0rd
EOF
kubectl cp $credentials_file openvpn-client:/tmp/login.conf

# Run in pod:
kubectl cp $clientpod_ovpn_file openvpn-client:/tmp/client.ovpn
kubectl exec openvpn-client -- apt update -y
kubectl exec openvpn-client -- apt install curl openvpn psmisc -y
kubectl exec openvpn-client -- mkdir -p /dev/net
kubectl exec openvpn-client -- mknod /dev/net/tun c 10 200
kubectl exec openvpn-client -- chmod 600 /dev/net/tun
kubectl exec openvpn-client -- cat /dev/net/tun
kubectl exec openvpn-client -- ls -al /tmp/client.ovpn

# Start VPN in client pod
kubectl exec openvpn-client -- openvpn --config /tmp/client.ovpn --auth-user-pass /tmp/login.conf --daemon --log-append /var/log/syslog

# Troubleshoot client pod
kubectl exec openvpn-client -- ip a
kubectl exec openvpn-client -- ip route

# Troubleshoot server pod
openvpn_server_pod=$(k get pod -l app.kubernetes.io/instance=openvpnserver -o json | jq -r '.items[0].metadata.name') && echo $openvpn_server_pod
kubectl exec $openvpn_server_pod -- sysctl net.ipv4.ip_forward
kubectl exec $openvpn_server_pod -- ip a
kubectl exec $openvpn_server_pod -- ip route

# Stop VPN in client pod
kubectl exec openvpn-client -- killall openvpn

# Create client VM in a new VNet
vpnclient_vm_name=vpnclientvm
vpnclient_vnet_name=vpnclientvm
vpnclient_vnet_prefix=172.16.31.0/24
vpnclient_subnet_name=vpnclientvm
vpnclient_subnet_prefix=172.16.31.0/26
vpnclient_pip_name="${vpnclient_vm_name}-pip"
vpnclient_vm_sku=Standard_B1s
echo "Creating VNet and subnet..."
az network vnet create -g $rg --name $vpnclient_vnet_name --address-prefix $vpnclient_subnet_prefix --subnet-name $vpnclient_subnet_name --subnet-prefix $vpnclient_subnet_prefix -o none
echo "Creating VM..."
az vm create -n $vpnclient_vm_name -g $rg --image UbuntuLTS --generate-ssh-keys --size $vpnclient_vm_sku -l $location \
   --vnet-name $vpnclient_vnet_name --subnet $vpnclient_subnet_name --public-ip-address $vpnclient_pip_name --public-ip-sku Standard -o none
vpnclient_pip_address=$(az network public-ip show -n $vpnclient_pip_name -g $rg --query ipAddress -o tsv) && echo $vpnclient_pip_address
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vpnclient_pip_address "ip a"

# Install openvpn client and copy ovpn profile (see OpenVPN section)
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vpnclient_pip_address "sudo apt-get install openvpn -y"
user=$(whoami)
scp $clientvm_ovpn_file ${vpnclient_pip_address}:/home/${user}/client.ovpn
scp $credentials_file ${vpnclient_pip_address}:/home/${user}/login.conf
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vpnclient_pip_address "ls -al"
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vpnclient_pip_address "sudo openvpn --config /home/${user}/client.ovpn --auth-user-pass /home/${user}/login.conf --daemon --log-append /var/log/syslog"

# Install sample app
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vpnclient_pip_address "sudo apt update && sudo apt install -y python3-pip"
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vpnclient_pip_address "sudo pip3 install flask"
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vpnclient_pip_address "sudo wget https://raw.githubusercontent.com/erjosito/azcli/master/myip.py -O /root/myip.py"
ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $vpnclient_pip_address "sudo python3 /root/myip.py >/root/myip.log 2>&1 &"


##########################
#  Azure Firewall (WIP)  #
##########################

# Variables
azfw_policy_name=aksfwpolicy
azfw_name=aksazfw
azfw_pip_name=${azfw_name}-pip
# Create policy and FW
az network vnet subnet create -g $rg -n AzureFirewallSubnet --vnet-name $vnet_name --address-prefix $azfw_subnet_prefix
az network firewall policy create -n $azfw_policy_name -g $rg
az network public-ip create -g $rg -n $azfw_pip_name --sku standard --allocation-method static -l $location
azfw_ip=$(az network public-ip show -g $rg -n $azfw_pip_name --query ipAddress -o tsv) && echo $azfw_ip
az network firewall create -n $azfw_name -g $rg -l $location
azfw_id=$(az network firewall show -n $azfw_name -g $rg -o tsv --query id)
az network firewall ip-config create -f $azfw_name -n azfw-ipconfig -g $rg --public-ip-address $azfw_pip_name --vnet-name $vnet_name
az network firewall update -n $azfw_name -g $rg
azfw_private_ip=$(az network firewall show -n $azfw_name -g $rg -o tsv --query 'ipConfigurations[0].privateIpAddress') && echo $azfw_private_ip
# Enable logging
logws_name=$(az monitor log-analytics workspace list -g $rg --query '[0].name' -o tsv)
if [[ -z "$logws_name" ]]
then
    logws_name=log$RANDOM
    echo "INFO: Creating log analytics workspace ${logws_name}..."
    az monitor log-analytics workspace create -n $logws_name -g $rg
else
    echo "INFO: Log Analytics workspace $logws_name found in resource group $rg"
fi
logws_id=$(az resource list -g $rg -n $logws_name --query '[].id' -o tsv)
logws_customerid=$(az monitor log-analytics workspace show -n $logws_name -g $rg --query customerId -o tsv)
az monitor diagnostic-settings create -n mydiag --resource $azfw_id --workspace $logws_id \
      --metrics '[{"category": "AllMetrics", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false }, "timeGrain": null}]' \
      --logs '[{"category": "AzureFirewallApplicationRule", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}, 
              {"category": "AzureFirewallNetworkRule", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}]' >/dev/null
# Add some rules to the policy (TBD)

# Cleanup
az network firewall delete -n $azfw_name -g $rg -y

###########################
# Logs from Azure Monitor #
###########################

logws_customerid=$(az monitor log-analytics workspace show -n $logws_name -g $rg --query customerId -o tsv)
# Log category summary
summary='AzureDiagnostics
| summarize count() by Category'
az monitor log-analytics query -w $logws_customerid --analytics-query $summary -o tsv
# Cluster autoscaler (see https://docs.microsoft.com/en-us/azure/aks/view-master-logs)
ca_query='AzureDiagnostics
| where Category == "cluster-autoscaler"
//| project log_s
| take 20 '
az monitor log-analytics query -w $logws_customerid --analytics-query $ca_query -o tsv
# Kube scheduler
sched_query='AzureDiagnostics
| where Category == "kube-scheduler"
//| project log_s
| take 20 '
az monitor log-analytics query -w $logws_customerid --analytics-query $sched_query -o tsv
# audit-admin (from our IP)
myip=$(curl -s4 ifconfig.co)
admin_query='AzureDiagnostics
| where TimeGenerated > ago(1d) 
| where Category == "kube-audit-admin"
| extend log_j = parse_json(log_s)
| extend log_kind = log_j.kind, log_verb = log_j.verb, log_sourceIP = log_j.sourceIPs[0], log_username= log_j.user.username
| extend log_object_resource=log_j.objectRef.resource, log_object_subresource=log_j.objectRef.subresource, log_object_name=log_j.objectRef.name, log_object_namespace=log_j.objectRef.namespace
| project TimeGenerated, log_sourceIP, log_kind, log_verb, log_username, log_object_resource,log_object_subresource,log_object_name, log_object_namespace
| where log_sourceIP == "'$myip'"
| take 20 '
az monitor log-analytics query -w $logws_customerid --analytics-query $admin_query -o tsv
# Insights query
insights_query='InsightsMetrics 
//| where Namespace == "kube-system"
//| extend TagInfo = parse_json(Tags)
//| where TagInfo.app !startswith "istio"
//| where TagInfo.app !startswith "cert"
//| where TagInfo.app != ""
| summarize count() by Namespace'
az monitor log-analytics query -w $logws_customerid --analytics-query $insights_query -o tsv
# ACR repo query
acr_repo_query='ContainerRegistryRepositoryEvents
| summarize count() by Repository'
az monitor log-analytics query -w $logws_customerid --analytics-query $acr_repo_query -o table
# ACR auth query
acr_auth_query='ContainerRegistryLoginEvents
| summarize count() by ResultDescription'
acr_auth_query='ContainerRegistryLoginEvents
| where ResultDescription == "401"
| project TimeGenerated, LoginServer, ResultDescription, CallerIpAddress'
az monitor log-analytics query -w $logws_customerid --analytics-query $acr_auth_query -o table
# Flow Logs (this example is for outbound flows)
flowlogs_query="NTANetAnalytics
| where TimeGenerated > ago(1h)
| where FlowType == 'AzurePublic' or FlowType == 'ExternalPublic'
| where isnotempty(SrcIp)
| where SrcSubnet =~ '${rg}/${vnet_name}/${aks_subnet_name}'
| project SrcSubnet, SrcIp, DestPort, DestPublicIPInfo=split(DestPublicIps, ' ')
| mv-expand DestPublicIPInfo
| extend DestPublicIP = tostring(split(DestPublicIPInfo, '|')[0])
| extend OutboundBytes = toint(split(DestPublicIPInfo, '|')[5]), InboundBytes = toint(split(DestPublicIPInfo, '|')[6])
| project-away DestPublicIPInfo
| lookup kind=leftouter (NTAIpDetails | project Ip, PublicIpDetails) on \$left.DestPublicIP==\$right.Ip
| summarize OutboundBytes=sum(OutboundBytes), InboundBtyes=sum(InboundBytes) by SrcSubnet, SrcIp, DestPublicIP, DestPort, PublicIpDetails"
az monitor log-analytics query -w $logws_customerid --analytics-query $flowlogs_query -o tsv
