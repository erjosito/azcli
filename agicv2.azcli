# AGICv2 (aka kubic, aka traffic controller, aka AppGW for containers, aka AGC)
# There are 2 steps to create AGC:
# 1. Create ALB Controller: https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/quickstart-deploy-application-gateway-for-containers-alb-controller?tabs=install-helm-windows
# 2. Create AGC
#    Option a - Managed: https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/quickstart-create-application-gateway-for-containers-managed-by-alb-controller?tabs=new-subnet-aks-vnet
#    Option b - BYOD: https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/quickstart-create-application-gateway-for-containers-byo-deployment?tabs=existing-vnet-subnet

# Control
mode=byod                        # managed or byod
cni=overlay                      # azure_pod_subnet, azure or overlay
tls=no                           # yes or no
alb_controller_version=1.4.12    # Check the latest version in https://learn.microsoft.com/azure/application-gateway/for-containers/alb-controller-release-notes
test_cmds=no                     # yes or no

# Variables
rg=agc3
location=eastasia                # eastasia, eastus, etc
aks_name=agc
aks_node_size=Standard_B2ms
vnet_name=aksVnet
vnet_prefix=10.13.0.0/16
aks_subnet_name=aks1stpool
aks_subnet_prefix=10.13.76.0/24  # Min /25 with Azure CNI!
pod_subnet_name=pods
pod_subnet_prefix=10.13.80.0/24
aks_service_cidr=10.0.0.0/16
aks_id_name=aksid
alb_id_name=albid
alb_deployment_ns=alb
alb_controller_ns=azure-alb-system
alb_ns_name=alb-infra
app_ns=yada
agc_name=appgw4c
agc_frontend_name=agc-frontend
agc_subnet_name=subnet-alb
agc_subnet_prefix=10.13.100.0/24
# TLS
pem_file="$HOME/onedrive/Admin/Certs/cloudtrooper.net/2025/star_cloudtrooper_net_fullchain.pem"
key_file="$HOME/onedrive/Admin/Certs/cloudtrooper.net/2025/CSR/star_cloudtrooper_net.key"
secret_name=cloudtrooper-tls
tls_app_fqdn=yada.cloudtrooper.net
tls_app_ns=yadatls

# Register required resource providers on Azure.
echo "Registering resource providers..."
az provider register --namespace Microsoft.ContainerService
az provider register --namespace Microsoft.Network
az provider register --namespace Microsoft.NetworkFunction
az provider register --namespace Microsoft.ServiceNetworking

# Install Azure CLI extensions.
function install_azcli_extension() {
    extension_name=$1
    extension_version=$(az extension show -n $extension_name --query version -o tsv 2>/dev/null)
    if [[ -z "$extension_version" ]]
    then
        echo "Azure CLI extension $extension_name not found, installing now..."
        az extension add -n $extension_name -o none
    else
        echo "Azure CLI extension $extension_name found with version $extension_version, trying to upgrade..."
        az extension update -n $extension_name -o none
    fi
    extension_version=$(az extension show -n $extension_name --query version -o tsv 2>/dev/null)
    echo "Azure CLI extension $extension_name installed with version $extension_version"
}
install_azcli_extension alb
install_azcli_extension aks-preview

# Create RG and VNets
echo "Creating resource group $rg and VNet $vnet_name in $location..."
az group create -n $rg -l $location -o none --only-show-errors
az network vnet create -g $rg -n $vnet_name --address-prefix $vnet_prefix -l $location -o none
az network vnet subnet create -g $rg -n $aks_subnet_name --vnet-name $vnet_name --address-prefix $aks_subnet_prefix -o none --only-show-errors
az network vnet subnet create -g $rg -n $pod_subnet_name --vnet-name $vnet_name --address-prefix $pod_subnet_prefix -o none --only-show-errors
aks_subnet_id=$(az network vnet subnet show -n $aks_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)
pod_subnet_id=$(az network vnet subnet show -n $pod_subnet_name --vnet-name $vnet_name -g $rg --query id -o tsv)

# Create/retrieve identity for AKS
id_id=$(az identity show -n $aks_id_name -g $rg --query id -o tsv 2>/dev/null)
if [[ -z "$id_id" ]]
then
    echo "Identity $aks_id_name not found, creating a new one..."
    az identity create -n $aks_id_name -g $rg -o none
    id_id=$(az identity show -n $aks_id_name -g $rg --query id -o tsv)
else
    echo "Identity $aks_id_name found with ID $id_id"
fi
id_principal_id=$(az identity show -n $aks_id_name -g $rg --query principalId -o tsv)
vnet_id=$(az network vnet show -n $vnet_name -g $rg --query id -o tsv)
sleep 15 # Time for creation to propagate
echo "Assigning Contributor role to AKS identity $aks_id_name in VNet $vnet_name..."
az role assignment create --scope $vnet_id --assignee $id_principal_id --role Contributor -o none

# Create AKS cluster with workload identity enabled.
# 'AGC supports Azure CNI Pod Subnet for both dynamic and static allocation': https://github.com/Azure/AKS/issues/4681
if [[ "$cni" == "azure_pod_subnet" ]]; then
    cni_options="--network-plugin azure --network-policy azure --vnet-subnet-id $aks_subnet_id --pod-subnet $pod_subnet_prefix"
elif [[ "$cni" == "azure_pod_subnet" ]]; then
    cni_options="--network-plugin azure --network-policy azure --vnet-subnet-id $aks_subnet_id"
elif [[ "$cni" == "overlay" ]]; then
    cni_options="--network-plugin azure --network-policy cilium --network-dataplane cilium --network-plugin-mode overlay"
else  # Default to Azure without pod subnet
    cni_options="--network-plugin azure --vnet-subnet-id $aks_subnet_id"
fi
# Create cluster
echo "Creating AKS cluster $aks_name in resource group $rg in $cni CNI mode..."
az aks create -g $rg -n $aks_name -l $location -o none \
    --enable-oidc-issuer --enable-workload-identity \
    -c 1 -s $aks_node_size --generate-ssh-keys -u $(whoami) --service-cidr $aks_service_cidr \
    --enable-managed-identity --assign-identity $id_id \
    ${(z)cni_options} \
    --load-balancer-sku Standard \
    --node-resource-group "$aks_name"-iaas-"$RANDOM"
# For existing clusters
# az aks update -g $rg -n $aks_name --enable-oidc-issuer --enable-workload-identity -o none

# Verify cluster created with right plugin
if [[ "$test_cmds" == "yes" ]]; then
    az aks list -g $rg --query '[].{Name:name, Plugin:networkProfile.networkPlugin, Dataplane:networkProfile.networkDataplane, Policy:networkProfile.networkPolicy}' -o table
fi

# Create identity for AGC and configure federation with AKS OIDC issuer
node_rg=$(az aks show -n $aks_name -g $rg --query "nodeResourceGroup" -o tsv)
node_rg_id=$(az group show -n $node_rg --query id -o tsv)
rg_id=$(az group show -n $rg --query id -o tsv)
echo "Creating ALB identity $alb_id_name in resource group $rg..."
az identity create -n $alb_id_name -g $rg -o none --only-show-errors
alb_id_principal_id="$(az identity show -n $alb_id_name -g $rg --query principalId -o tsv)"
echo "Waiting 15 seconds to allow for replication of the identity..."
sleep 15
echo "Applying Reader role to the AKS resource groups for the newly provisioned identity..."
reader_role=acdd72a7-3385-48ef-bd42-f606fba81ae7
az role assignment create --assignee-object-id $alb_id_principal_id --assignee-principal-type ServicePrincipal --role $reader_role --scope $node_rg_id -o none --only-show-errors
az role assignment create --assignee-object-id $alb_id_principal_id --assignee-principal-type ServicePrincipal --role $reader_role --scope $rg_id -o none --only-show-errors
echo "Setting up federation with AKS OIDC issuer..."
aks_oidc_issuer="$(az aks show -n "$aks_name" -g "$rg" --query "oidcIssuerProfile.issuerUrl" -o tsv)"
az identity federated-credential create -n $alb_id_name -g $rg -o none --only-show-errors \
    --identity-name $alb_id_name --issuer "$aks_oidc_issuer" --subject "system:serviceaccount:azure-alb-system:alb-controller-sa"

# Deploy Helm chart
# Notice the version is hard-coded, you can update it with 'helm upgrade'
echo "Deploying Helm chart for ALB controller"
az aks get-credentials -n $aks_name -g $rg --overwrite-existing
kubectl create ns $alb_deployment_ns
# The --namespace flag doesnt seem to have any effect
helm install alb-controller oci://mcr.microsoft.com/application-lb/charts/alb-controller \
      --namespace $alb_deployment_ns \
      --version $alb_controller_version \
      --set albController.namespace=$alb_controller_ns \
      --set albController.podIdentity.clientID=$(az identity show -g $rg -n $alb_id_name --query clientId -o tsv)

# Verify
if [[ "$test_cmds" == "yes" ]]; then
    kubectl get pods -n $alb_controller_ns
    echo "Waiting 20 seconds for the ALB controller to start..."
    sleep 20
    kubectl get gatewayclass azure-alb-external -o yaml
fi

#############################
#          Managed          #
#############################
if [[ "$mode" == "managed" ]]; then
    # Create AppGW for containers
    echo "Creating subnet for AGC..."
    az network vnet subnet create -g $rg --vnet-name $vnet_name -n $agc_subnet_name --address-prefix $agc_subnet_prefix --delegations 'Microsoft.ServiceNetworking/trafficControllers' -o none --only-show-errors
    agc_subnet_id=$(az network vnet subnet show -g $rg --vnet-name $vnet_name -n $agc_subnet_name --query id --output tsv)

    # Delegate roles to the ALB identity
    echo "Adding roles for ALB identity..."
    node_rg=$(az aks show -n $aks_name -g $rg --query "nodeResourceGroup" -o tsv)
    node_rg_id=$(az group show -n $node_rg --query id -o tsv)
    rg_id=$(az group show -n $rg --query id -o tsv)
    az role assignment create --assignee-object-id $alb_id_principal_id --assignee-principal-type ServicePrincipal --scope $node_rg_id --role "fbc52c3f-28ad-4303-a892-8a056630b8f1" -o none  # AppGw for Containers Configuration Manager role
    az role assignment create --assignee-object-id $alb_id_principal_id --assignee-principal-type ServicePrincipal --scope $rg_id --role "fbc52c3f-28ad-4303-a892-8a056630b8f1" -o none  # AppGw for Containers Configuration Manager role
    az role assignment create --assignee-object-id $alb_id_principal_id --assignee-principal-type ServicePrincipal --scope $agc_subnet_id --role "4d97b98b-1d4f-4787-a291-c67834d212e7" -o none # Network Contributor

    # Create ALB with subnet association
    echo "Creating AGC and frontend in managed mode..."
  kubectl create ns $alb_ns_name
  kubectl apply -f - <<EOF
apiVersion: alb.networking.azure.io/v1
kind: ApplicationLoadBalancer
metadata:
  name: $agc_name
  namespace: $alb_ns_name
spec:
  associations:
  - $agc_subnet_id
EOF
    # kubectl get ApplicationLoadBalancer -n $alb_ns_name
    # kubectl describe ApplicationLoadBalancer -n $alb_ns_name

    # Create Gateway in k8s
    alb_id=$(az network alb list -g $node_rg --query '[0].id' -o tsv)
    alb_name=$(az network alb list -g $node_rg --query '[0].name' -o tsv)
    echo "Creating Gateway in k8s associated to ALB ID $alb_id..."
    if [[ "$tls" == "yes" ]]; then
        kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: $agc_name
  annotations:
    alb.networking.azure.io/alb-namespace: $alb_ns_name
    alb.networking.azure.io/alb-id: $alb_id
spec:
  gatewayClassName: azure-alb-external
  listeners:
  - name: http
    port: 80
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: All
  - name: https
    port: 443
    protocol: HTTPS
    allowedRoutes:
      namespaces:
        from: All
    tls:
      mode: Terminate
      certificateRefs:
      - kind : Secret
        group: ""
        name: $secret_name
EOF
    else
        kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: $agc_name
  annotations:
    alb.networking.azure.io/alb-namespace: $alb_ns_name
    alb.networking.azure.io/alb-id: $alb_id
spec:
  gatewayClassName: azure-alb-external
  listeners:
  - name: http
    port: 80
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: All
EOF
    fi

    if [[ "$test_cmds" == "yes" ]]; then
        kubectl get gateway
    fi
fi

#############################
# Bring your own deployment #
#############################

if [[ "$mode" == "byod" ]]; then
    # Create ALB and frontend
    echo "Creating AGC and frontend in Bring-Your-Own-Deployment mode..."
    az network alb create -g $rg -n $agc_name -o none --only-show-errors
    az network alb frontend create -g $rg --alb-name $agc_name -n $agc_frontend_name -o none --only-show-errors
    az network vnet subnet create -g $rg --vnet-name $vnet_name -n $agc_subnet_name --address-prefix $agc_subnet_prefix --delegations 'Microsoft.ServiceNetworking/trafficControllers' -o none --only-show-errors
    agc_subnet_id=$(az network vnet subnet show -g $rg --vnet-name $vnet_name -n $agc_subnet_name --query id --output tsv)

    # Delegate roles to the ALB identity
    echo "Adding roles for ALB identity..."
    az role assignment create --assignee-object-id $alb_id_principal_id --assignee-principal-type ServicePrincipal --scope $node_rg_id --role "fbc52c3f-28ad-4303-a892-8a056630b8f1" -o none  # AppGw for Containers Configuration Manager role
    az role assignment create --assignee-object-id $alb_id_principal_id --assignee-principal-type ServicePrincipal --scope $rg_id --role "fbc52c3f-28ad-4303-a892-8a056630b8f1" -o none  # AppGw for Containers Configuration Manager role
    az role assignment create --assignee-object-id $alb_id_principal_id --assignee-principal-type ServicePrincipal --scope $agc_subnet_id --role "4d97b98b-1d4f-4787-a291-c67834d212e7" -o none # Network Contributor

    # Associate the AppGW4C with the subnet
    echo "Associating the AGC with the subnet..."
    association_name='AppGW4Cassociation'
    az network alb association create -g $rg -n $association_name --alb-name $agc_name --subnet $agc_subnet_id -o none --only-show-errors

    # Create Gateway in k8s
    echo "Creating gateway in k8s..."
    agc_id=$(az network alb list -g $rg --query '[0].id' -o tsv)
    # agc_name=$(az network alb list -g $rg --query '[0].name' -o tsv)
    # agc_frontend_name=$(az network alb frontend list --alb-name $agc_name -g $rg --query '[0].name' -o tsv)
    echo "Creating Gateway in k8s associated to ALB ID $agc_id and frontend $agc_frontend_name..."
    if [[ "$tls" == "yes" ]]; then
        kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  name: $agc_name
  annotations:
    alb.networking.azure.io/alb-namespace: $alb_ns_name
    alb.networking.azure.io/alb-id: $agc_id
spec:
  gatewayClassName: azure-alb-external
  listeners:
  - name: http
    port: 80
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: All
  - name: https
    port: 443
    protocol: HTTPS
    allowedRoutes:
      namespaces:
        from: All
        # from: Selector
        # selector:
        #   matchLabels:
        #     kubernetes.io/metadata.name: $tls_app_ns
    tls:
      mode: Terminate
      certificateRefs:
      - kind : Secret
        group: ""
        name: $secret_name
  addresses:
  - type: alb.networking.azure.io/alb-frontend
    value: $agc_frontend_name
EOF
    else
        kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  name: $agc_name
  annotations:
    alb.networking.azure.io/alb-namespace: $alb_ns_name
    alb.networking.azure.io/alb-id: $agc_id
spec:
  gatewayClassName: azure-alb-external
  listeners:
  - name: http
    port: 80
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: All
  addresses:
  - type: alb.networking.azure.io/alb-frontend
    value: $agc_frontend_name
EOF
    fi

    if [[ "$test_cmds" == "yes" ]]; then
        kubectl get gateway
        kubectl describe gateway
    fi
fi

##################################
#       Diagnostic settings      #
##################################

# Create log analytics workspace
# ToDo: get the name of AGC, it could be different if created using the managed deployment
logws_name=$(az monitor log-analytics workspace list -g $rg --query '[0].name' -o tsv)
if [[ -z "$logws_name" ]]
then
    logws_name=log$RANDOM
    echo "Creating new Log Analytics workspace $logws_name"
    az monitor log-analytics workspace create -n $logws_name -g $rg -o none
else
    echo "Log Analytics workspace $logws_name found in resource group $rg."
fi
echo "Getting the ID of Log Analytics workspace $logws_name..."
logws_id=$(az resource list -g $rg -n $logws_name --query '[].id' -o tsv)
logws_customerid=$(az monitor log-analytics workspace show -n $logws_name -g $rg --query customerId -o tsv)
# Enable diagnostics settings for AGC
echo "Looking for AGC in the main resource group $rg..."
agc_id=$(az network alb list -g $rg --query '[0].id' -o tsv)
if [[ -z "$agc_id" ]]
then
    echo "AGC not found in main resource group, looking for AGC in node resource group $node_rg..."
    agc_id=$(az network alb list -g $rg --query '[0].id' -o tsv)
fi
if [[ -z "$agc_id" ]]; then
    echo "AGC not found in node resource group $node_rg, exiting..."
else
    echo "AGC found with ID $agc_id, configuring diagnostics settings..."
    az monitor diagnostic-settings create --name agc-diagnostics --resource $agc_id --workspace $logws_id -o none --only-show-errors \
      --logs '[{"category": "TrafficControllerAccessLog", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}]'
fi

# Get logs
query="AGCAccessLogs | where TimeGenerated > ago(5m) | project TimeGenerated, ClientIp, HostName, RequestUri, FrontendName, FrontendPort, BackendHost, BackendIp, HttpStatusCode"
if [[ "$test_cmds" == "yes" ]]; then
    az monitor log-analytics query -w $logws_customerid --analytics-query "$query" -o table
fi

####################################
# Wait until gateway is programmed #
####################################

wait_interval=15
max_wait_time=600
echo "Waiting for gateway $agc_name to finish provisioning..."
start_time=`date +%s`
state=$(kubectl get gateway $agc_name -o json | jq -r '.status.conditions[] | select(.type=="Programmed") | .status')
run_time=0
until [[ "$state" == "True" ]] || [[ run_time -gt $max_wait_time ]] || [[ -z "$state" ]]; do
  sleep $wait_interval
  state=$(kubectl get gateway $agc_name -o json | jq -r '.status.conditions[] | select(.type=="Programmed") | .status')
  run_time=$(expr `date +%s` - $start_time)
done
if [[ -z "$state" ]]
then
  echo "I could not retrieve the state of the gateway $agc_name"
else
  ((minutes=${run_time}/60))
  ((seconds=${run_time}%60))
  echo "Gateway Programmed state is $state, wait time $minutes minutes and $seconds seconds"
fi

##################################
#         Frontend TLS           #
##################################

if [[ "$tls" == "yes" ]]; then
    echo "Creating secret $secret_name for frontend TLS from file $pem_file..."
    kubectl create secret tls $secret_name --cert=$pem_file --key=$key_file
    echo "Creating FrontendTLSPolicy for gateway $agc_name..."
    # A TLS policy only needed if you need MTLS, maybe we could create a frontend policy for other purposes such as TLS version?
    #    According to the docs, only mTLS supported: https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/api-specification-kubernetes#alb.networking.azure.io/v1.FrontendTLSPolicyConfig
#     kubectl apply -f - <<EOF
# apiVersion: alb.networking.azure.io/v1
# kind: FrontendTLSPolicy
# metadata:
#   name: mtls-policy
#   namespace: default
# spec:
#   targetRef:
#     group: gateway.networking.k8s.io
#     kind: Gateway
#     name: $agc_name
#     namespace: default
#     sectionNames:
#     - mtls-listener
#   default:
#     verify:
#       caCertificateRef:
#         name: ca.bundle
#         group: ""
#         kind: Secret
#         namespace: $secret_name
# EOF
fi

##################################
# Sample workload in the cluster #
##################################

# See https://gateway-api.sigs.k8s.io/guides/multiple-ns/
# See https://github.com/microsoft/YADA/tree/main/api

echo "Creating sample app in namespace $app_ns"
kubectl create ns $app_ns
kubectl create deployment yadaapi -n $app_ns --image=erjosito/yadaapi:1.0 --port=8080 --replicas=1
kubectl expose deploy yadaapi -n $app_ns --port=8080 --target-port=8080
# kubectl apply -f - <<EOF
# apiVersion: v1
# kind: Service
# metadata:
#   annotations:
#     service.beta.kubernetes.io/azure-load-balancer-internal: "true"
#   name: yadaapi
#   namespace: $app_ns
# spec:
#   selector:
#     app: yadaapi
#   type: ClusterIP
#   ports:
#   - port: 8080
# EOF

# HTTP route
echo "Creating HTTPRoute for sample app..."
fqdn=$(kubectl get gateway $agc_name -o jsonpath='{.status.addresses[0].value}') && echo "The frontend FQDN is $fqdn"
kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1beta1
kind: HTTPRoute
metadata:
  name: yadaapi
  namespace: $app_ns
spec:
  parentRefs:
  - name: $agc_name
    namespace: default
    sectionName: http
  hostnames:
  - "$fqdn"
  rules:
  - backendRefs:
    - name: yadaapi
      namespace: $app_ns
      port: 8080
EOF

# Health check policy
echo "Creating health check policy for sample app..."
kubectl apply -f - <<EOF
apiVersion: alb.networking.azure.io/v1
kind: HealthCheckPolicy
metadata:
  name: healthcheck-yada-active
  namespace: $app_ns
spec:
  targetRef:
    group: ""
    kind: Service
    namespace: $app_ns
    name: yadaapi
  default:
    interval: 5s
    timeout: 3s
    healthyThreshold: 1
    unhealthyThreshold: 1
    port: 8080
    http:
      host: contoso.com
      path: /api/healthcheck
      match:
        statusCodes: 
        - start: 200
          end: 299
    useTLS: false
EOF

if [[ "$test_cmds" == "yes" ]]; then
    # Check
    kubectl get httproute
    kubectl describe httproute yadaapi

    # Scale out
    kubectl scale deploy yadaapi -n $app_ns --replicas=2

    # Test
    curl "http://${fqdn}/api/healthcheck"
    curl "http://${fqdn}/api/headers"
    curl "http://${fqdn}/api/ip"
    while true; do mydate=$(date); echo -n "${mydate}: "; curl -s "http://${fqdn}/api/ip" | jq -r '.my_private_ip'; sleep 1; done
    while true; do mydate=$(date); echo -n "${mydate}: "; ip=$(curl -s "http://${fqdn}/api/ip" | jq -r '.my_private_ip'); [[ -z "$ip" ]] && echo "No answer" || echo "$ip"; sleep 1; done
fi

#####################
#  Backup service   #
#####################

kubectl create deployment yadaapi-backup -n $app_ns --image=erjosito/yadaapi:1.0 --port=8080 --replicas=1
kubectl expose deploy yadaapi-backup -n $app_ns --port=8080 --target-port=8080

# HTTP route
fqdn=$(kubectl get gateway $agc_name -o jsonpath='{.status.addresses[0].value}') && echo $fqdn
kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: yadaapi
  namespace: $app_ns
spec:
  parentRefs:
  - name: $agc_name
    namespace: default
    sectionName: http
  hostnames:
  - "$fqdn"
  rules:
  - backendRefs:
    - name: yadaapi
      port: 8080
      weight: 999
    - name: yadaapi-backup
      port: 8080
      weight: 1
EOF

# Health check policy
kubectl apply -f - <<EOF
apiVersion: alb.networking.azure.io/v1
kind: HealthCheckPolicy
metadata:
  name: healthcheck-yada-backup
  namespace: $app_ns
spec:
  targetRef:
    group: ""
    kind: Service
    namespace: $app_ns
    name: yadaapi-backup
  default:
    interval: 5s
    timeout: 3s
    healthyThreshold: 1
    unhealthyThreshold: 1
    port: 8080
    http:
      host: contoso.com
      path: /api/healthcheck
      match:
        statusCodes: 
        - start: 200
          end: 299
    useTLS: false
EOF


#####################
#    TLS service    #
#####################

###### WIP!!! #######

if [[ $tls == "yes" ]]; then

    # Create namespace
    kubectl create ns $tls_app_ns

    # Create nginx secret
    # ssl_crt=$(cat $pem_file | base64)
    # ssl_key=$(cat $key_file | base64)
    kubectl create secret tls yadatls-cert --cert=$pem_file --key=$key_file -n $tls_app_ns

    # Create deployment
    kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-conf
  namespace: $tls_app_ns
data:
  nginx.conf: |
    user nginx;
    worker_processes auto;
    events {
      worker_connections 1024;
    }
    pid /var/run/nginx.pid;
    http {
        server {
            listen 443 ssl;
            server_name localhost;
            ssl_protocols              TLSv1.2;
            ssl_ciphers                ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:ECDHE-RSA-RC4-SHA:ECDHE-ECDSA-RC4-SHA:AES128:AES256:RC4-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!3DES:!MD5:!PSK;
            ssl_prefer_server_ciphers  on;
            ssl_session_cache    shared:SSL:10m; # a 1mb cache can hold about 4000 sessions, so we can hold 40000 sessions
            ssl_session_timeout  24h;
            keepalive_timeout 300; # up from 75 secs default
            add_header Strict-Transport-Security 'max-age=31536000; includeSubDomains';
            ssl_certificate /etc/nginx/ssl/tls.crt;
            ssl_certificate_key /etc/nginx/ssl/tls.key; 
            # ssl_certificate      /etc/nginx/ssl.crt;
            # ssl_certificate_key  /etc/nginx/ssl.key;
            location /api/ {
                proxy_pass http://127.0.0.1:8080 ;
                proxy_set_header Connection "";
                proxy_set_header Host \$host;
                proxy_set_header X-Real-IP \$remote_addr;
                proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
                # proxy_set_header X-Forwarded-For \$remote_addr;
            }
        }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: yadatls
  name: yadatls
  namespace: $tls_app_ns
spec:
  replicas: 1
  selector:
    matchLabels:
      run: yadatls
  template:
    metadata:
      labels:
        run: yadatls
    spec:
      containers:
      - name: api
        image: erjosito/yadaapi:1.0
        ports:
        - containerPort: 8080
          protocol: TCP
      - name: nginx
        image: mcr.microsoft.com/oss/nginx/nginx:1.15.5-alpine
        ports:
        - containerPort: 443
          protocol: TCP
        volumeMounts:
            - name: nginx-conf
              mountPath: /etc/nginx/nginx.conf
              subPath: nginx.conf
              readOnly: true
            - name: certs
              mountPath: /etc/nginx/ssl
      volumes:
      - name: nginx-conf
        configMap:
          name: nginx-conf
          items:
            - key: nginx.conf
              path: nginx.conf
      - name: certs
        secret:
           secretName: yadatls-cert
      restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: yadatls8080
  namespace: $tls_app_ns
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
  selector:
    run: yadatls
---
apiVersion: v1
kind: Service
metadata:
  name: yadatls443
  namespace: $tls_app_ns
spec:
  type: ClusterIP
  ports:
  - port: 443
    targetPort: 443
  selector:
    run: yadatls
---
apiVersion: alb.networking.azure.io/v1
kind: BackendTLSPolicy
metadata:
  name: yadatlspolicy
  namespace: $tls_app_ns
spec:
  targetRef:
    group: ""
    kind: Service
    name: yadatls443
    namespace: $tls_app_ns
  default:
    sni: $tls_app_fqdn
    ports:
    - port: 443
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: tlsyadaapi
  namespace: $tls_app_ns
spec:
  parentRefs:
  - name: $agc_name
    namespace: default
    sectionName: https
  hostnames:
  - "$tls_app_fqdn"
  rules:
  - backendRefs:
    - name: yadatls443
      port: 443
---
apiVersion: alb.networking.azure.io/v1
kind: HealthCheckPolicy
metadata:
  name: healthcheck-yada-tls
  namespace: $tls_app_ns
spec:
  targetRef:
    group: ""
    kind: Service
    namespace: $tls_app_ns
    name: yadatls443
  default:
    interval: 5s
    timeout: 3s
    healthyThreshold: 1
    unhealthyThreshold: 1
    port: 443
    http:
      host: contoso.com
      path: /api/healthcheck
      match:
        statusCodes: 
        - start: 200
          end: 299
    useTLS: true
EOF

    ip_address=$(dig +short $fqdn)
    if [[ "$test_cmds" == "yes" ]]; then
        curl --resolve "${tls_app_fqdn}:443:${ip_address}" "https://${tls_app_fqdn}/api/ip"
    fi
fi

#####################
# Azure Diagnostics #
#####################

agc_name=$(az network alb list -g $node_rg --query '[0].name' -o tsv)
if [[ "$test_cmds" == "yes" ]]; then
    az network alb list -g $node_rg -o table
    az network alb association list --alb-name $agc_name -g $node_rg -o table
    az network alb frontend list --alb-name $agc_name -g $node_rg -o table
fi

##########################
# Kubernetes Diagnostics #
##########################

if [[ "$test_cmds" == "yes" ]]; then
    # Check the agic pods
    kubectl get pods -n azure-alb-system
    # Verify the ALB in Azure
    az network alb list -g $rg -o table
    alb_name=$(az network alb list -g $rg --query '[0].name' -o tsv)
    az network alb frontend list -g $rg --alb-name $alb_name -o table
    # Verify the ALB in k8s
    kubectl get applicationLoadBalancer
    # Verify the gatewayclass
    kubectl get gatewayclass azure-alb-external -o yaml
    # Gateways
    kubectl get gateway
    kubectl describe gateway
    # HTTP routes
    kubectl get httproute
fi