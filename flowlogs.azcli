########################################################
# Script to deploy VMs and generate traffic between them.
# NSG flow logs and Traffic Analytics can be used to 
#    explore the traffic flows
#
# Jose Moreno
# May 2021
########################################################

# Control
create_azfw=yes
create_adx=yes
create_elk=yes

# Variables
rg=flowlogs
location=eastus2
vnet_name=flowlogs
vnet_prefix=192.168.0.0/16
subnet0_name=vm0
subnet0_prefix=192.168.0.0/24
subnet1_name=vm1
subnet1_prefix=192.168.1.0/24
nsg_name=flowlogs
cloudinit_file="/tmp/cloudinit.txt"
vm_size=Standard_B1s
no_of_vms=4
flows="1;2:3;4:1433:1000,1:2:80:100,3:4:443:100"  # Syntax: src:dst:port:kb/min

# AzFW variables
azfw_subnet_prefix=192.168.10.0/24
azfw_name=myazfw
azfw_policy_name=myazfwpolicy
azfw_pip_name=myazfw-pip

# ADX variables
adx_sku='Dev(No SLA)_Standard_E2a_v4'
adx_tier='Basic'     # Not able to make it work with other values such as Developer/Basic
adx_capacity=1
adx_name='nwlogs$RANDOM'

# ElasticStack variables
elk_subnet_prefix=192.168.20.0/24
elk_subnet_name=elastic
elk_vm_name=elkvm
elk_vm_size=Standard_B2ms
elk_cloudinit_file=/tmp/elk_cloudinit.txt

# Some helper functions

# Converts a list to a shell array
function convert_string_to_array () {
    # Default to comma as separator
    if [[ -n $2 ]]
    then
        separator=$2
    else
        separator=','
    fi
    # Different syntax for bash and zsh
    if [ -n "$BASH_VERSION" ]; then
        arr_opt=a
    elif [ -n "$ZSH_VERSION" ]; then
        arr_opt=A
    fi
    # Do the split into array
    IFS=$separator read -r"${arr_opt}" myarray <<< "$1"
    echo "${myarray[@]}"
}

# Configure flow logs for all NSGs in a given location
function configure_flowlogs () {
    if [[ -z $1 ]]; then
        nsg_location=$(az group show -n $rg --query location -o tsv)
        echo "No argument specified, taking the RG's location $nsg_location"
    else
        echo "Using $1 as location"
        nsg_location=$1
    fi
    storage_account_name=$(az storage account list -g $rg -o tsv --query "[?location=='$nsg_location'].name" | head -1)
    if [[ -z "$storage_account_name" ]]; then
        storage_account_name=$(echo "logs$RANDOM${nsg_location}" | cut -c1-24)  # max 24 characters
        echo "No storage account found in $nsg_location, creating one..."
        az storage account create -n $storage_account_name -g $rg --sku Standard_LRS --kind StorageV2 -l $nsg_location -o none
    else
        echo "Storage account $storage_account_name created in $nsg_location, using it for NSG flow flogs"
    fi
    echo "Looking for NSGs in resource group $rg in location $nsg_location..."
    nsg_list=$(az network nsg list -g $rg -o tsv --query "[?location=='$nsg_location'].name")
    echo "$(echo $nsg_list | wc -l) NSGs found"
    while IFS= read -r nsg_name; do
        echo "Configuring Flow Logs for NSG $nsg_name into storage account $storage_account_name..."
        az network watcher flow-log create -l $nsg_location -n "${nsg_name}-${nsg_location}" -g $rg --nsg $nsg_name --storage-account $storage_account_name --log-version 2 --retention 1 -o none
    done <<< "$nsg_list"
}

# Set some variables getting the content from AKV
function get_secrets() {
    # Variables
    akv_name=erjositoKeyvault
    default_password_secret=defaultPassword
    # Get secrets
    akv_rg_found=$(az keyvault list -o tsv --query "[?name=='$akv_name'].resourceGroup" 2>/dev/null)
    if [[ -n ${akv_rg_found} ]]
    then
        echo "INFO: AKV ${akv_name} found in resource group $akv_rg_found"
        default_password=$(az keyvault secret show --vault-name $akv_name -n $default_password_secret --query 'value' -o tsv 2>/dev/null)
    else
        echo "ERROR: secrets could not be read because Azure Key Vault ${akv_name} could not be found"
    fi
}

# Create RG, Vnet, NSG
az group create -n $rg -l $location -o none
az network vnet create -g $rg -n $vnet_name --address-prefix $vnet_prefix --subnet-name $subnet1_name --subnet-prefix $subnet1_prefix -l $location -o none
az network vnet subnet create --vnet-name $vnet_name --name $subnet0_name -g $rg --address-prefixes $subnet0_prefix -o none
az network nsg create -n $nsg_name -g $rg -l $location -o none
az network nsg rule create -n allowSSHin --nsg-name $nsg_name -g $rg --priority 1000 --destination-port-ranges 22 --access Allow --protocol Tcp -o none
az network vnet subnet update -n $subnet1_name --vnet-name $vnet_name -g $rg --nsg $nsg_name -o none
az network vnet subnet update -n $subnet0_name --vnet-name $vnet_name -g $rg --nsg $nsg_name -o none

# Create Log Analytics workspace
logws_name=$(az monitor log-analytics workspace list -g $rg --query '[].name' -o tsv 2>/dev/null)  # Retrieve the WS name if it already existed
if [[ -z "$logws_name" ]]
then
    logws_name=log$RANDOM
    az monitor log-analytics workspace create -n $logws_name -g $rg -o none
fi
logws_id=$(az resource list -g $rg -n $logws_name --query '[].id' -o tsv)
logws_customerid=$(az monitor log-analytics workspace show -n $logws_name -g $rg --query customerId -o tsv)

# Create storage account
storage_account_name=$(az storage account list -g $rg --query '[].name' -o tsv 2>/dev/null)  # Retrieve the storage account name if it already existed
if [[ -z "$storage_account_name" ]]
then
    storage_account_name=log$RANDOM
    az storage account create -n $storage_account_name -g $rg --sku Standard_LRS --kind StorageV2 -l $location -o none
fi

# Enable flow logs
az network watcher flow-log create -l $location -n "flowlog-$location" -g $rg \
    --nsg $nsg_name --storage-account $storage_account_name --log-version 2 --retention 7 \
    --workspace $logws_id --interval 10 --traffic-analytics true -o none

# Generate cloudinit file to create VMs
cat <<EOF > $cloudinit_file
#cloud-config
packages:
  - jq
  - pv
EOF

# Create VMs spread over two subnets
for i in {1..$no_of_vms}
do
    vm_name="vm$(printf "%02d" i)"
    subnet_index=$(expr $i % 2)
    az vm create -n $vm_name -g $rg --image UbuntuLTS --generate-ssh-keys --size $vm_size \
    --vnet-name $vnet_name --subnet "vm${subnet_index}" --public-ip-address "${vm_name}-pip" --nsg $nsg_name \
    --custom-data $cloudinit_file -l $location --public-ip-sku Standard --no-wait
done

# Wait some seconds and create JSON with required IPs
sleep 60
ip_json=$(az vm list-ip-addresses -g $rg -o json)

# Get the private IP of a specific VM out of the output of the command "az vm list-ip-addresses"
function get_private_ip () {
    echo $1 | jq -r '.[] | select(.virtualMachine.name == "'$2'") | .virtualMachine.network.privateIpAddresses[0]'
}

# Get the public IP of a specific VM out of the output of the command "az vm list-ip-addresses"
function get_public_ip () {
    echo $1 | jq -r '.[] | select(.virtualMachine.name == "'$2'") | .virtualMachine.network.publicIpAddresses[0].ipAddress'
}

# Azure Firewall
if [[ "$create_azfw" == "yes" ]]; then
    # Start
    az network vnet subnet create --vnet-name $vnet_name --name AzureFirewallSubnet -g $rg --address-prefixes $azfw_subnet_prefix -o none
    az network public-ip create -g $rg -n $azfw_pip_name --sku standard --allocation-method static -l $location -o none
    azfw_ip=$(az network public-ip show -g $rg -n $azfw_pip_name --query ipAddress -o tsv)
    az network firewall policy create -n $azfw_policy_name -g $rg
    az network firewall policy rule-collection-group create -n ruleset01 --policy-name $azfw_policy_name -g $rg --priority 1000
    # Allow SSH and HTTP for connection monitor (uses TCP9 too)
    echo "Creating rule to allow SSH and HTTP..."
    az network firewall policy rule-collection-group collection add-filter-collection --policy-name $azfw_policy_name --rule-collection-group-name ruleset01 -g $rg \
        --name mgmt --collection-priority 101 --action Allow --rule-name allowSSHnHTTP --rule-type NetworkRule --description "TCP 22" \
        --destination-addresses 10.0.0.0/8 172.16.0.0/12 20.0.0.0/6 --source-addresses 10.0.0.0/8 172.16.0.0/12 20.0.0.0/6 --ip-protocols TCP --destination-ports 9 22 80 -o none
    # Allow ICMP
    echo "Creating rule to allow ICMP..."
    az network firewall policy rule-collection-group collection add-filter-collection --policy-name $azfw_policy_name --rule-collection-group-name ruleset01 -g $rg \
        --name icmp --collection-priority 102 --action Allow --rule-name allowICMP --rule-type NetworkRule --description "ICMP traffic" \
        --destination-addresses 10.0.0.0/8 172.16.0.0/12 20.0.0.0/6 --source-addresses 10.0.0.0/8 172.16.0.0/12 20.0.0.0/6 --ip-protocols ICMP --destination-ports "1-65535" >/dev/null
    # Allow NTP
    echo "Creating rule to allow NTP..."
    az network firewall policy rule-collection-group collection add-filter-collection --policy-name $azfw_policy_name --rule-collection-group-name ruleset01 -g $rg \
        --name ntp --collection-priority 103 --action Allow --rule-name allowNTP --rule-type NetworkRule --description "Egress NTP traffic" \
        --destination-addresses '*' --source-addresses "10.0.0.0/8" "20.0.0.0/6" --ip-protocols UDP --destination-ports "123" -o none
    # Example application collection with 2 rules (ipconfig.co, api.ipify.org)
    echo "Creating rule to allow ifconfig.co and api.ipify.org..."
    az network firewall policy rule-collection-group collection add-filter-collection --policy-name $azfw_policy_name --rule-collection-group-name ruleset01 -g $rg \
        --name ifconfig --collection-priority 201 --action Allow --rule-name allowIfconfig --rule-type ApplicationRule --description "ifconfig" \
        --target-fqdns "ifconfig.co" --source-addresses "10.0.0.0/8" "172.16.0.0/12" "20.0.0.0/6" --protocols Http=80 Https=443 -o none
    az network firewall policy rule-collection-group collection rule add -g $rg --policy-name $azfw_policy_name --rule-collection-group-name ruleset01 --collection-name ifconfig \
        --name ipify --target-fqdns "api.ipify.org" --source-addresses "10.0.0.0/8" "172.16.0.0/12" "20.0.0.0/6" --protocols Http=80 Https=443 --rule-type ApplicationRule -o none
    # Example application collection with wildcards (*.ubuntu.com)
    echo "Creating rule to allow *.ubuntu.com..."
    az network firewall policy rule-collection-group collection add-filter-collection --policy-name $azfw_policy_name --rule-collection-group-name ruleset01 -g $rg \
        --name ubuntu --collection-priority 202 --action Allow --rule-name repos --rule-type ApplicationRule --description "ubuntucom" \
        --target-fqdns 'ubuntu.com' '*.ubuntu.com' --source-addresses '*' --protocols Http=80 Https=443 -o none
    # Mgmt traffic to Azure
    az network firewall policy rule-collection-group collection add-filter-collection --policy-name $azfw_policy_name --rule-collection-group-name ruleset01 -g $rg \
        --name azure --collection-priority 203 --action Allow --rule-name azmonitor --rule-type ApplicationRule --description "Azure Monitor" \
        --target-fqdns '*.opinsights.azure.com' '*.azure-automation.net' --source-addresses '*' --protocols Https=443 -o none
    # Create Azure Firewall
    echo "Creating Azure Firewall..."
    az network firewall create -n $azfw_name -g $rg --policy $azfw_policy_name -l $location -o none
    # Configuring IP
    echo "Configuring firewall logs and private IP..."
    azfw_id=$(az network firewall show -n $azfw_name -g $rg -o tsv --query id)
    az monitor diagnostic-settings create -n mydiag --resource $azfw_id --workspace $logws_id \
        --metrics '[{"category": "AllMetrics", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false }, "timeGrain": null}]' \
        --logs '[{"category": "AzureFirewallApplicationRule", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}, 
                {"category": "AzureFirewallNetworkRule", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}]' -o none
    az network firewall ip-config create -f $azfw_name -n azfw-ipconfig -g $rg --public-ip-address $azfw_pip_name --vnet-name $vnet_name -o none
    az network firewall update -n $azfw_name -g $rg -o none
    azfw_private_ip=$(az network firewall show -n $azfw_name -g $rg -o tsv --query 'ipConfigurations[0].privateIpAddress') && echo "$azfw_private_ip"
    # Create route tables
    az network route-table create --name subnet0 --resource-group "$rg" --location "$location" -o none
    az network route-table route create --route-table-name subnet0 -g $rg --address-prefix $subnet1_prefix --name subnet1 --next-hop-type VirtualAppliance --next-hop-ip-address $azfw_private_ip -o none
    az network vnet subnet update --name $subnet0_name --route-table subnet0 --vnet-name $vnet_name --resource-group $rg -o none
    az network route-table create --name subnet1 --resource-group "$rg" --location "$location" -o none
    az network route-table route create  --route-table-name subnet1 -g $rg --address-prefix $subnet0_prefix --name subnet0 --next-hop-type VirtualAppliance --next-hop-ip-address $azfw_private_ip -o none
    az network vnet subnet update --name $subnet1_name --route-table subnet1 --vnet-name $vnet_name --resource-group $rg -o none

fi

# Aux function to add/update extension
function add_extension() {
    extension_name=$1
    extension_version=$(az extension show -n $extension_name --query version -o tsv 2>/dev/null)
    if [[ -z "$extension_version" ]]
    then
        echo "Azure CLI extension $extension_name not found, installing now..."
        az extension add -n $extension_name -o none
    else
        echo "Azure CLI extension $extension_name found with version $extension_version, trying to upgrade..."
        az extension update -n $extension_name -o none
    fi
    extension_version=$(az extension show -n $extension_name --query version -o tsv 2>/dev/null)
    echo "Azure CLI extension $extension_name installed with version $extension_version"
}

# Azure Data eXplorer (optional)
# WORK IN PROGRESS!!!!
if [[ "$create_adx" == "yes" ]]; then
    # Create ADX cluster
    add_extension kusto
    az kusto cluster create -n $adx_name --sku "name=$adx_sku" "tier=$adx_tier" "capacity=$adx_capacity" -g $rg -l $location -o none
    az kusto database create --cluster-name $adx_name --database-name $adx_name -g $rg --read-write-database soft-delete-period=P365D hot-cache-period=P31D location=$location -o none
    adx_id=$(az kusto cluster show -n $adx_name -g $rg --query id -o tsv)
    adx_principal_id=$(az kusto cluster show -n $adx_name -g $rg --query 'identity.principalId' -o tsv)
    # Create event grid
    add_extension eventgrid
    storage_account_name=$(az storage account list -g $rg -o tsv --query "[?location=='$location'].name" | head -1)
    storage_account_id=$(az storage account show -n $storage_account_name -g $rg --query id --output tsv)
    eg_name=$storage_account_name
    az eventgrid system-topic create -n $eg_name -g $rg -l $location --topic-type microsoft.storage.storageaccounts --source $storage_account_id -o none
    eg_id=$(az eventgrid system-topic show -n $eg_name -g $rg --query id -o tsv)
    az role assignment create --scope $eg_id --assignee $adx_principal_id --role 'EventGrid EventSubscription Reader' -o none
    az role assignment create --scope $storage_account_id --assignee $adx_principal_id --role 'Storage Blob Data Contributor' -o none
    kql_query='.create table TestTable (TimeStamp: datetime, Value: string, Source:string)'
    az kusto script create --cluster-name $adx_name -g $rg --database-name $adx_name --continue-on-errors true -n createTable --script-content "$kql_query" -o none
    kql_query=".create table TestTable ingestion json mapping 'TestMapping' '[{\"column\":\"Records\",\"path\":\"$.records\"},{\"column\":\"Value\",\"path\":\"$.Value\"},{\"column\":\"Source\",\"path\":\"$.Source\"}]'"
    az kusto script create --cluster-name $adx_name -g $rg --database-name $adx_name --continue-on-errors true -n createTable --script-content "$kql_query" -o none
    az kusto data-connection event-grid create --cluster-name $adx_name -n $eg_name --database-name $adx_name -l $location \
         --event-grid-resource-id $eg_id --blob-storage-event-type 'Microsoft.Storage.BlobCreated' --storage-account-resource-id $storage_account_id \
         --data-format JSON --table-name flowlogs --managed-identity-resource-id $adx_id -g $rg -o none
    # Create Event Hubs namespace
    eh_name=$(az eventhubs namespace list -g $rg --query '[0].name' -o tsv)
    eh_ns_name=$eh_name
    if [[ -z "$eh_ns_name" ]]; then
        eh_name=nwlogs$RANDOM
        eh_ns_name=$eh_name
        echo "Creating Event Hub ${eh_name}..."
        az eventhubs namespace create -n $eh_ns_name -g $rg -l $location --sku Standard -o none
        az eventhubs eventhub create -n $eh_name -g $rg --namespace-name $eh_ns_name -o none
    else
        echo "Events Hub $eh_name found in RG $rg"
    fi
    eh_ns_cx_string=$(az eventhubs namespace authorization-rule keys list -g $rg --namespace-name $eh_ns_name --name RootManageSharedAccessKey --query primaryConnectionString -o tsv)
    eh_ns_id=$(az eventhubs namespace show -g $rg -n $eh_ns_name --query id -o tsv)
    eh_id=$(az eventhubs eventhub show --namespace-name $eh_ns_name -g $rg -n $eh_name --query id -o tsv)
    az role assignment create --scope $eh_ns_id --assignee $adx_principal_id --role 'Azure Event Hubs Data Receiver' -o none
    # az kusto data-connection event-hub data-connection-validation --cluster-name $adx_name -n $eh_name --database-name $adx_name -l $location \
    #      --consumer-group '$Default' --event-hub-resource-id $eh_id --managed-identity-resource-id $adx_id -g $rg -o none
    az kusto data-connection event-hub create --cluster-name $adx_name -n $eh_name --database-name $adx_name -l $location \
         --consumer-group '$Default' --event-hub-resource-id $eh_id --managed-identity-resource-id $adx_id -g $rg -o none
    # Add diag setting to AzFW to send logs to this Event hub
    if [[ "$create_azfw" == "yes" ]]; then
        azfw_id=$(az network firewall show -n $azfw_name -g $rg -o tsv --query id)
        az monitor diagnostic-settings create -n $eh_name --resource $azfw_id --event-hub $eh_ns_id --event-hub-rule RootManageSharedAccessKey \
            --metrics '[{"category": "AllMetrics", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false }, "timeGrain": null}]' \
            --logs '[{"category": "AZFWNetworkRule", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
                    {"category": "AZFWApplicationRule", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}, 
                    {"category": "AZFWNatRule", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
                    {"category": "AZFWThreatIntel", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
                    {"category": "AZFWIdpsSignature", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
                    {"category": "AZFWDnsQuery", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
                    {"category": "AZFWFqdnResolveFailure", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
                    {"category": "AZFWFatFlow", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}},
                    {"category": "AZFWFlowTrace", "enabled": true, "retentionPolicy": {"days": 0, "enabled": false}}]' -o none
    fi
fi

# Optional: create ElasticStack VM
# WORK IN PROGRESS!!!!
if [[ "$create_elk" == "yes" ]]; then
    # Create Subnet
    echo "Creating subnet $elk_subnet_name ($elk_subnet_prefix)..."
    az network vnet subnet create --vnet-name $vnet_name --name $elk_subnet_name -g $rg --address-prefixes $elk_subnet_prefix -o none
    # Create NSGs
    echo "Creating NSG for Elastic VM..."
    az network nsg create -n "${elk_vm_name}-nsg" -g $rg -o none
    az network nsg rule create -n SSH --nsg-name "${elk_vm_name}-nsg" -g $rg --priority 1000 --destination-port-ranges 22 --access Allow --protocol Tcp -o none
    az network nsg rule create -n Kibana --nsg-name "${elk_vm_name}-nsg" -g $rg --priority 1010 --destination-port-ranges 5601 --access Allow --protocol Tcp -o none
    az network nsg rule create -n ElasticSearch --nsg-name "${elk_vm_name}-nsg" -g $rg --priority 1020 --destination-port-ranges 9200 --access Allow --protocol Tcp -o none
    az network nsg rule create -n ICMP --nsg-name "${elk_vm_name}-nsg" -g $rg --priority 1030 --source-address-prefixes '*' --destination-address-prefixes '*' --destination-port-ranges '*' --access Allow --protocol Icmp -o none
    # Create Elastic Search VM
    echo "Creating cloudinit file for Elastic Search..."
    cat <<EOF > $elk_cloudinit_file
#cloud-config
packages:
- jq
- apt-transport-https
runcmd:
- wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -
- echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | tee /etc/apt/sources.list.d/elastic-8.x.list
- apt update && apt install -y elasticsearch kibana logstash
- apt install -y filebeat metricbeat
- systemctl enable elasticsearch
- systemctl enable kibana
- systemctl start elasticsearch
- systemctl start kibana
EOF
    echo "Creating Elastic Stack VM..."
    az vm create -n $elk_vm_name -g $rg -l $location --image ubuntuLTS --generate-ssh-keys --nsg "${elk_vm_name}-nsg" -o none \
        --custom-data $elk_cloudinit_file --public-ip-sku Standard --public-ip-address "${elk_vm_name}-pip" \
        --vnet-name $vnet_name --size $elk_vm_size --subnet $elk_subnet_name -l $location
    # Make sure to reset the elastic user's password with elasticsearch-setup-password or elasticsearch-reset-password !!!!
    echo "Sleeping now until Elastic gets installed..."
    sleep 300
    echo "You will be prompted now to reset the Elastic and Kibana passwords. Please enter the value of the defaultPassword variable/secret (see above in the script):"
    elk_pip=$(az network public-ip show -n "${elk_vm_name}-pip" -g $rg --query ipAddress -o tsv)
    ssh $elk_pip "sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic -i"
    ssh $elk_pip "sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u kibana -i"
    # Verify Elastic Search VM
    if [[ -z "$default_password" ]]; then
        echo "ERROR: default_password is empty. Please enter the value of the defaultPassword variable/secret (see above in the script):"
        read default_password
    fi
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "systemctl status elasticsearch"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "systemctl status kibana"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "curl -sk -u elastic:${default_password} https://localhost:9200"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "curl -sk -u kibana_system:${default_password} https://localhost:9200/_xpack/security/_authenticate?pretty"  # Not working!
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "curl -sk -u elastic:${default_password} https://localhost:9200/_cluster/health"
    # Configure ElasticSearch and Kibana
    echo "Configuring ElasticSearch and Kibana..."
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo sed -i '/network.host/c\network.host: 0.0.0.0' /etc/elasticsearch/elasticsearch.yml"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo systemctl restart elasticsearch"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo sed -i '/server.host/c\server.host: \"0.0.0.0\"' /etc/kibana/kibana.yml"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo sed -i '/server.name/c\server.name: \"elastictest\"' /etc/kibana/kibana.yml"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo sed -i '/elasticsearch.username/c\elasticsearch.username: \"kibana\"' /etc/kibana/kibana.yml"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo sed -i '/elasticsearch.password/c\elasticsearch.password: \"$default_password\"' /etc/kibana/kibana.yml"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo sed -i '/elasticsearch.hosts/c\elasticsearch.hosts: [\"https://localhost:9200\"]' /etc/kibana/kibana.yml"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo sed -i '/elasticsearch.ssl.verificationMode/c\elasticsearch.ssl.verificationMode: none' /etc/kibana/kibana.yml"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo systemctl restart kibana"
    # Install logstash plugin
    # ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo /usr/share/logstash/bin/logstash-plugin install logstash-input-azureblob"  # Errors out!
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo /usr/share/logstash/bin/logstash-plugin install logstash-input-azure_blob_storage"
    # Configure logstash
    storage_account_name=$(az storage account list -g $rg --query '[].name' -o tsv 2>/dev/null)  # Retrieve the storage account name if it already existed
    storage_account_key=$(az storage account keys list -n $storage_account_name --query '[0].value' -o tsv)
    # From https://learn.microsoft.com/en-us/azure/network-watcher/network-watcher-visualize-nsg-flow-logs-open-source-tools
    cat <<EOF > /tmp/logstash.conf
input {
   azure_blob_storage {
        storageaccount => "$storage_account_name"
        access_key => "$storage_account_key"
        container => "insights-logs-networksecuritygroupflowevent"
        codec => "json"
        file_head => '{"records":['
        file_tail => ']}'
        interval => 300
        path_filters => ['**/*.json']
        # Enable / tweak these settings when event is too big for codec to handle.
        # break_json_down_policy => "with_head_tail"
        # break_json_batch_count => 2
    }
}
filter {
    split { field => "[records]" }
    split { field => "[records][properties][flows]" }
    split { field => "[records][properties][flows][flows]" }
    split { field => "[records][properties][flows][flows][flowTuples]" }
    mutate {
        split => { "[records][resourceId]" => "/"}
        add_field => {
            "Subscription" => "%{[records][resourceId][2]}"
            "ResourceGroup" => "%{[records][resourceId][4]}"
            "NetworkSecurityGroup" => "%{[records][resourceId][8]}"
        }
        convert => {"Subscription" => "string"}
        convert => {"ResourceGroup" => "string"}
        convert => {"NetworkSecurityGroup" => "string"}
        split => { "[records][properties][flows][flows][flowTuples]" => ","}
        add_field => {
            "unixtimestamp" => "%{[records][properties][flows][flows][flowTuples][0]}"
            "srcIp" => "%{[records][properties][flows][flows][flowTuples][1]}"
            "destIp" => "%{[records][properties][flows][flows][flowTuples][2]}"
            "srcPort" => "%{[records][properties][flows][flows][flowTuples][3]}"
            "destPort" => "%{[records][properties][flows][flows][flowTuples][4]}"
            "protocol" => "%{[records][properties][flows][flows][flowTuples][5]}"
            "trafficflow" => "%{[records][properties][flows][flows][flowTuples][6]}"
            "traffic" => "%{[records][properties][flows][flows][flowTuples][7]}"
            "flowstate" => "%{[records][properties][flows][flows][flowTuples][8]}"
            "packetsSourceToDest" => "%{[records][properties][flows][flows][flowTuples][9]}"
            "bytesSentSourceToDest" => "%{[records][properties][flows][flows][flowTuples][10]}"
            "packetsDestToSource" => "%{[records][properties][flows][flows][flowTuples][11]}"
            "bytesSentDestToSource" => "%{[records][properties][flows][flows][flowTuples][12]}"
        }
        convert => {"unixtimestamp" => "integer"}
        convert => {"srcPort" => "integer"}
        convert => {"destPort" => "integer"}
        convert => {"packetsSourceToDest" => "integer"}
        convert => {"bytesSourceToDest" => "integer"}
        convert => {"packetsDestToSource" => "integer"}
        convert => {"bytesSentDestToSource" => "integer"}
    }
    date {
        match => ["unixtimestamp" , "UNIX"]
    }
}
output {
    elasticsearch {
        hosts => ["https://localhost:9200"]
        index => "nsg-flow-logs-%{+xxxx.ww}"
        ssl => true 
        ssl_certificate_verification => false 
        user => "elastic" 
        password => "$default_password"
    }
}  
EOF
    # From https://github.com/janmg/logstash-input-azure_blob_storage
    cat <<EOF > /tmp/logstash.conf
input {
    azure_blob_storage {
        codec => "json"
        storageaccount => "$storage_account_name"
        access_key => "$storage_account_key"
        container => "insights-logs-networksecuritygroupflowevent"
        logtype => "nsgflowlog"
        prefix => "resourceId=/"
        path_filters => ['**/*.json']
        addfilename => true
        interval => 300
        debug_timer => true
        debug_until => 100
    }
}
filter {
    json {
        source => "message"
    }
}
output {
    stdout { codec => rubydebug }
    elasticsearch {
        hosts => ["https://localhost:9200"]
        index => "nsg-flow-logs-%{+xxxx.ww}"
        ssl => true 
        ssl_certificate_verification => false 
        user => "elastic" 
        password => "$default_password"
    }
}  
EOF
    user=$(whoami)
    scp /tmp/logstash.conf $elk_pip:/home/$user/logstash.conf
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo mv /home/${user}/logstash.conf /etc/logstash/conf.d/logstash.conf"
    # Start logstash
    # ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo systemctl restart logstash"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo systemctl start logstash"
    ssh -n -o BatchMode=yes -o StrictHostKeyChecking=no $elk_pip "sudo systemctl enable logstash"
    # Import object API (to use with .ndjson files, not exactly json)
    # curl -sk -u elastic:${default_password} -X POST -H "kbn-xsrf: true" --form file=@/tmp/visualization.ndjson "http://${elk_pip}:5601/api/saved_objects/_import"
    # curl -sk -u elastic:${default_password} -X POST -H "kbn-xsrf: true" --form file=@/tmp/dashboard.json "http://${elk_pip}:5601/api/saved_objects/_import"
    # Export object API (here an example for data views aka index-patterns)
    # curl -sk -u elastic:${default_password} -X POST -H "kbn-xsrf: true" -H 'Content-Type: application/json' "http://${elk_pip}:5601/api/saved_objects/_export" -d '{"type": "index-pattern"}'
    # curl -sk -u elastic:${default_password} -X POST -H "kbn-xsrf: true" -H 'Content-Type: application/json' "http://${elk_pip}:5601/api/saved_objects/_export" -d '{"type": "dashboard"}'
    # URLs
    echo "Browse to http://$elk_pip:5601 for Kibana (the username is 'elastic')"
fi

# Start traffic generation
flows_array=($(convert_string_to_array $flows ','))
for flow in "${flows_array[@]}"
do
    # echo "Processing flow $flow..."
    sources=$(echo $flow | cut -d':' -f 1)
    destinations=$(echo $flow | cut -d':' -f 2)
    port=$(echo $flow | cut -d':' -f 3)
    kb_min=$(echo $flow | cut -d':' -f 4)
    src_array=($(convert_string_to_array $sources ';'))
    dst_array=($(convert_string_to_array $destinations ';'))
    for dst in $dst_array; do
        # Start nc listening on port for destination
        dst_vm_name="vm$(printf "%02d" $dst)"
        dst_pip=$(get_public_ip $ip_json "$dst_vm_name")
        dst_ip=$(get_private_ip $ip_json "$dst_vm_name")
        echo "Running \"nc -dlk -p ${port}\" on ${dst_vm_name}, ${dst_pip}"
        ssh -n -o StrictHostKeyChecking=no -o BatchMode=yes "$dst_pip" "nc -dlk -p $port > /dev/null &"
        for src in $src_array; do
            # Configure crontab entry to send traffic every minute
            src_vm_name="vm$(printf "%02d" $src)"
            src_pip=$(get_public_ip $ip_json "$src_vm_name")
            cmd='(crontab -l 2>/dev/null; echo "* * * * * dd if=/dev/urandom bs=1000 count='${kb_min}' | pv -L 10M | nc '${dst_ip}' '${port}'") | crontab -'
            echo "Adding crontab entry for ${src_vm_name}, ${src_pip}"
            ssh -n -o StrictHostKeyChecking=no -o BatchMode=yes "$src_pip" "$cmd"
        done
    done
done

# Diagnostics
az network watcher flow-log list -o table -l $location
src_vm_name=vm01
dst_vm_name=vm03
port=1433
src_pip=$(get_public_ip $ip_json "$src_vm_name")
dst_pip=$(get_public_ip $ip_json "$dst_vm_name")
dst_ip=$(get_private_ip $ip_json "$dst_vm_name")
ssh -n -o StrictHostKeyChecking=no -o BatchMode=yes "$src_pip" "crontab -l | grep \"$dst_ip $port\""
ssh -n -o StrictHostKeyChecking=no -o BatchMode=yes "$src_pip" "nc -vz $dst_ip $port"
ssh -n -o StrictHostKeyChecking=no -o BatchMode=yes "$dst_pip" "ps -ef | grep \"nc -dlk -p $port\" | grep -v grep"
ssh -n -o StrictHostKeyChecking=no -o BatchMode=yes "$dst_pip" "sudo netstat -tunlp | grep \":$port\""

# Simulate port scan
src_vm_name=vm01
port=22
src_pip=$(get_public_ip $ip_json "$src_vm_name") && echo $src_pip
for i in {1..$no_of_vms}
do
    dst_vm_name="vm$(printf "%02d" i)"
    dst_ip=$(get_private_ip $ip_json "$dst_vm_name")
    ssh -n -o StrictHostKeyChecking=no -o BatchMode=yes "$src_pip" "nc -vz $dst_ip $port"
done

# ADX
az kusto cluster list -g $rg -o table
az kusto data-connection list --cluster-name $adx_name --database-name $adx_name -g $rg -o table
az eventgrid system-topic list -g $rg -o table

##############
# Stop/Start #
##############

function stop_firewall() {
    azfw_name=$(az network firewall list -g $rg --query '[0].name' -o tsv)
    azfw_ipconfig_name=$(az network firewall show -n $azfw_name -g $rg --query 'ipConfigurations[0].name' -o tsv)
    echo "Stoping Azure Firewall ${azfw_name}..."
    az network firewall ip-config delete -f $azfw_name -n azfw-ipconfig -g $rg -o none
    az network firewall update -n $azfw_name -g $rg -o none
}
function start_firewall() {
    azfw_name=$(az network firewall list -g $rg --query '[0].name' -o tsv)
    if [[ -n "$azfw_name" ]]; then
        echo "Starting Azure Firewall ${azfw_name}..."
        azfw_ipconfig_name="${azfw_name}-ipconfig"
        az network firewall ip-config create -f $azfw_name -n $azfw_ipconfig_name -g $rg --public-ip-address $azfw_pip_name --vnet-name $vnet_name -o none
        az network firewall update -n $azfw_name -g $rg -o none
    else
        echo "No Azure Firewall found in RG $rg"
    fi
}
function stop_adx() {
    adx_name=$(az kusto cluster list -g $rg --query '[0].name' -o tsv)
    if [[ -n "$adx_name" ]]; then
        echo "Stopping ADX cluster $adx_name..."
        az kusto cluster stop -n $adx_name -g $rg -o none
    else
        echo "No ADX cluster found in RG $rg"
    fi
}
function start_adx() {
    adx_name=$(az kusto cluster list -g $rg --query '[0].name' -o tsv)
    if [[ -n "$adx_name" ]]; then
        echo "Starting ADX cluster $adx_name..."
        az kusto cluster start -n $adx_name -g $rg -o none
    else
        echo "No ADX cluster found in RG $rg"
    fi
}
function stop_lab() {
    stop_vms
    stop_firewall
    stop_adx
}
function stop_vms() {
    vm_list=$(az vm list -o tsv -g "$rg" --query "[].name")
    while IFS= read -r vm_name; do
        echo "Deallocating Virtual Machine ${vm_name}..."
        az vm deallocate -g $rg -n "$vm_name" --no-wait -o none
    done <<< "$vm_list"
}
function start_vms() {
    vm_list=$(az vm list -o tsv -g "$rg" --query "[].name")
    while IFS= read -r vm_name; do
        echo "Starting Virtual Machine ${vm_name}..."
        az vm start -g $rg -n "$vm_name" --no-wait -o none
    done <<< "$vm_list"
}
function start_lab() {
    start_vms
    if [[ "$create_azfw" == "yes" ]]; then
        start_firewall
    fi
    if [[ "$create_adx" == "yes" ]]; then
        start_adx
    fi
}

###########
# Cleanup #
###########
az network watcher flow-log delete -l $location -n 